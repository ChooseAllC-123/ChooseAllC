{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286dc0db-fb20-4544-a001-202d1899770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edcffd-9ec1-4bf5-94a1-948e37f09f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged dataset: D:\\PhD\\RA\\Schafer\\IRA\\data\\merged_all_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set working directory\n",
    "directory = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\"\n",
    "\n",
    "# Step 2: Get all Excel files in the directory\n",
    "files = [f for f in os.listdir(directory) if f.endswith('.xlsx')]\n",
    "\n",
    "# Step 3: Read and append all files\n",
    "all_data = []\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_excel(file_path)\n",
    "    all_data.append(df)\n",
    "\n",
    "# Step 4: Merge all data into a single DataFrame\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Step 5: Save the merged dataset before deduplication\n",
    "merged_file = os.path.join(directory, \"merged_all_data.xlsx\")\n",
    "combined_df.to_excel(merged_file, index=False)\n",
    "print(f\"Saved merged dataset: {merged_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e5dbb-b8e4-4838-95fa-08050874527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Remove duplicate Drug IDs (keeping the first occurrence)\n",
    "deduplicated_df = combined_df.drop_duplicates(subset=['Citeline Drug ID'], keep='first')\n",
    "\n",
    "# Step 7: Save the final deduplicated dataset\n",
    "final_file = os.path.join(directory, \"final_deduplicated_data.xlsx\")\n",
    "deduplicated_df.to_excel(final_file, index=False)\n",
    "print(f\"Saved final deduplicated dataset: {final_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c72a13f-6cba-4c6e-bf5f-40ed1730708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the files are located\n",
    "directory = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\"\n",
    "\n",
    "# Define the file names\n",
    "file_name_20200616 = \"20200616.xlsx\"\n",
    "file_name_final_deduplicated = \"final_deduplicated_data.xlsx\"\n",
    "\n",
    "# Combine directory and file names to create full file paths\n",
    "file_path_20200616 = os.path.join(directory, file_name_20200616)\n",
    "file_path_final_deduplicated = os.path.join(directory, file_name_final_deduplicated)\n",
    "\n",
    "# Load the two Excel files\n",
    "df_20200616 = pd.read_excel(file_path_20200616)\n",
    "df_final_deduplicated = pd.read_excel(file_path_final_deduplicated)\n",
    "\n",
    "# Extract the Citeline Drug IDs from the final_deduplicated_data.xlsx\n",
    "drug_ids_to_remove = set(df_final_deduplicated['Citeline Drug ID'])\n",
    "\n",
    "# Filter rows in 20200616.xlsx to keep only those where Drug ID is NOT in drug_ids_to_remove\n",
    "filtered_df = df_20200616[~df_20200616['drugid'].isin(drug_ids_to_remove)]\n",
    "\n",
    "# Save the updated DataFrame back to a new file in the same directory\n",
    "output_file_name = \"20200616_filtered.xlsx\"\n",
    "output_file_path = os.path.join(directory, output_file_name)\n",
    "filtered_df.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b2731-4fb8-48cf-8b5b-8c9f42ba9d23",
   "metadata": {},
   "source": [
    "## split the new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "359c7940-ccdc-4b5f-9624-75b7c0b33b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation complete. Data saved to: D:/PhD/RA/Schafer/IRA/data/transformed\\transformed_data2.xlsx\n",
      "Mismatched event counts found for the following Citeline Drug IDs:\n",
      "71628\n",
      "245454\n",
      "134350\n",
      "183089\n",
      "64310\n",
      "70750\n",
      "163715\n",
      "75097\n",
      "292020\n",
      "147997\n",
      "34786\n",
      "290588\n",
      "291325\n",
      "33657\n",
      "89109\n",
      "69763\n",
      "160029\n",
      "38368\n",
      "114476\n",
      "42822\n",
      "125985\n",
      "126134\n",
      "178511\n",
      "117921\n",
      "31975\n",
      "89883\n",
      "85446\n",
      "86817\n",
      "36618\n",
      "25659\n",
      "43357\n",
      "111159\n",
      "90489\n",
      "169855\n",
      "77738\n",
      "80245\n",
      "141310\n",
      "113914\n",
      "210821\n",
      "202873\n",
      "200445\n",
      "68558\n",
      "62575\n",
      "29902\n",
      "111767\n",
      "120563\n",
      "132540\n",
      "139971\n",
      "91363\n",
      "72046\n",
      "68258\n",
      "28702\n",
      "28985\n",
      "38731\n",
      "131769\n",
      "187948\n",
      "252268\n",
      "36868\n",
      "13845\n",
      "15730\n",
      "247660\n",
      "151454\n",
      "3006\n",
      "278401\n",
      "37602\n",
      "27851\n",
      "247926\n",
      "14550\n",
      "82214\n",
      "34517\n",
      "235987\n",
      "10425\n",
      "115136\n",
      "222088\n",
      "42566\n",
      "16453\n",
      "23004\n",
      "123484\n",
      "33394\n",
      "150991\n",
      "229120\n",
      "74346\n",
      "219976\n",
      "239192\n",
      "163168\n",
      "61742\n",
      "74375\n",
      "190198\n",
      "208388\n",
      "253298\n",
      "238338\n",
      "202966\n",
      "155809\n",
      "129733\n",
      "35280\n",
      "68233\n",
      "17529\n",
      "122315\n",
      "148635\n",
      "230043\n",
      "25985\n",
      "19577\n",
      "190191\n",
      "148554\n",
      "214359\n",
      "81511\n",
      "32154\n",
      "226842\n",
      "229824\n",
      "28869\n",
      "197219\n",
      "187440\n",
      "174497\n",
      "153833\n",
      "36934\n",
      "161153\n",
      "203999\n",
      "32854\n",
      "167463\n",
      "35510\n",
      "85560\n",
      "199556\n",
      "63865\n",
      "179771\n",
      "42169\n",
      "187945\n",
      "81831\n",
      "76167\n",
      "177589\n",
      "294931\n",
      "295093\n",
      "88077\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: Set working directory and load file\n",
    "directory = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\merged\"\n",
    "file_name = \"final_deduplicated_data.xlsx\"\n",
    "file_path = os.path.join(directory, file_name)\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "active_statuses = [\n",
    "    'Launched',\n",
    "    'Phase I Clinical Trial',\n",
    "    'Phase II Clinical Trial',\n",
    "    'Phase III Clinical Trial',\n",
    "    'Preclinical',\n",
    "    'Pre-registration',\n",
    "    'No Development Reported'\n",
    "    'Registered'\n",
    "]\n",
    "\n",
    "# Filter only drugs with continued development\n",
    "df = df[df['Global Status'].isin(active_statuses)].copy()\n",
    "\n",
    "# Replace NaN values with empty strings in the event columns\n",
    "df['Event Date'] = df['Event Date'].fillna('').astype(str)\n",
    "df['Event Type'] = df['Event Type'].fillna('').astype(str)\n",
    "df['Event Details'] = df['Event Details'].fillna('').astype(str)\n",
    "\n",
    "# Split the Event Date, Event Type, and Event Details columns by newline (\\n)\n",
    "df['Event Date'] = df['Event Date'].apply(lambda x: re.split(r'\\r?\\n', x))\n",
    "df['Event Type'] = df['Event Type'].apply(lambda x: re.split(r'\\r?\\n', x))\n",
    "df['Event Details'] = df['Event Details'].apply(lambda x: re.split(r'\\r?\\n', x))\n",
    "\n",
    "# Create a new DataFrame to store the transformed data\n",
    "transformed_data = []\n",
    "\n",
    "# Create a list to track problematic Citeline Drug IDs\n",
    "problematic_ids = []\n",
    "\n",
    "# Iterate through each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the lists of Event Date, Event Type, and Event Details\n",
    "    event_dates = row['Event Date']\n",
    "    event_types = row['Event Type']\n",
    "    event_details = row['Event Details']\n",
    "\n",
    "    # Check for length mismatch\n",
    "    if not (len(event_dates) == len(event_types) == len(event_details)):\n",
    "        problematic_ids.append(row['Citeline Drug ID'])\n",
    "        continue  # Skip this row or optionally handle it with padding\n",
    "\n",
    "    # Create a new row for each event\n",
    "    for i in range(len(event_dates)):\n",
    "        new_row = row.copy()\n",
    "        new_row['Event Date'] = event_dates[i]\n",
    "        new_row['Event Type'] = event_types[i]\n",
    "        new_row['Event Details'] = event_details[i]\n",
    "        transformed_data.append(new_row)\n",
    "\n",
    "# Convert the transformed data into a DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_data)\n",
    "\n",
    "# Reset index\n",
    "transformed_df = transformed_df.reset_index(drop=True)\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"D:/PhD/RA/Schafer/IRA/data/transformed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save transformed data\n",
    "output_file_path = os.path.join(output_dir, \"transformed_data2.xlsx\")\n",
    "transformed_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(\"Transformation complete. Data saved to:\", output_file_path)\n",
    "\n",
    "# Report problematic rows\n",
    "if problematic_ids:\n",
    "    print(\"Mismatched event counts found for the following Citeline Drug IDs:\")\n",
    "    for pid in problematic_ids:\n",
    "        print(pid)\n",
    "\n",
    "    # Optionally save to Excel\n",
    "    pd.Series(problematic_ids, name='Citeline Drug ID').to_excel(\n",
    "        os.path.join(output_dir, \"problematic_ids.xlsx\"), index=False)\n",
    "else:\n",
    "    print(\"No mismatched rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6efd494-216e-4c21-a424-cecc5644bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed rows saved to: D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\fixed_problematic_ids.xlsx\n",
      "⚠️ Still unmatched rows saved to: D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\still_unfixed_problematic_ids.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Step 1: Setup paths ===\n",
    "input_path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\merged\\final_deduplicated_data.xlsx\"\n",
    "output_dir = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\"\n",
    "output_fixed = os.path.join(output_dir, \"fixed_problematic_ids.xlsx\")\n",
    "output_unfixed = os.path.join(output_dir, \"still_unfixed_problematic_ids.xlsx\")\n",
    "\n",
    "# === Step 2: Load data ===\n",
    "df = pd.read_excel(input_path)\n",
    "\n",
    "# === Step 3: Clean and prepare ===\n",
    "for col in ['Event Date', 'Event Type', 'Event Details']:\n",
    "    df[col] = df[col].fillna('').astype(str)\n",
    "\n",
    "# === Step 4: Load previously saved problematic IDs ===\n",
    "# Replace [...] with the actual list if not already in memory\n",
    "\n",
    "problematic_ids_path = os.path.join(output_dir, \"problematic_ids.xlsx\")\n",
    "problematic_ids = pd.read_excel(problematic_ids_path)['Citeline Drug ID'].tolist()\n",
    "\n",
    "# Filter the original dataset to only those rows\n",
    "df_problematic = df[df['Citeline Drug ID'].isin(problematic_ids)].copy()\n",
    "\n",
    "# === Step 5: Smart split for Event Details only ===\n",
    "def smart_split_details(text):\n",
    "    lines = re.split(r'\\r?\\n', text)\n",
    "    merged = []\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            merged.append('')\n",
    "            continue\n",
    "        if merged and line[0].islower() and merged[-1] != '':\n",
    "            merged[-1] += ' ' + line.strip()\n",
    "        else:\n",
    "            merged.append(line.strip())\n",
    "    return merged\n",
    "\n",
    "# === Step 6: Process each row ===\n",
    "fixed_data = []\n",
    "still_problematic = []\n",
    "\n",
    "for _, row in df_problematic.iterrows():\n",
    "    event_dates = re.split(r'\\r?\\n', row['Event Date'])\n",
    "    event_types = re.split(r'\\r?\\n', row['Event Type'])\n",
    "    event_details = smart_split_details(row['Event Details'])\n",
    "\n",
    "    if len(event_dates) == len(event_types) == len(event_details):\n",
    "        for i in range(len(event_dates)):\n",
    "            new_row = row.copy()\n",
    "            new_row['Event Date'] = event_dates[i]\n",
    "            new_row['Event Type'] = event_types[i]\n",
    "            new_row['Event Details'] = event_details[i]\n",
    "            fixed_data.append(new_row)\n",
    "    else:\n",
    "        still_problematic.append({\n",
    "            'Citeline Drug ID': row['Citeline Drug ID'],\n",
    "            'Event Date Count': len(event_dates),\n",
    "            'Event Type Count': len(event_types),\n",
    "            'Event Detail Count': len(event_details)\n",
    "        })\n",
    "\n",
    "# === Step 7: Save results ===\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fixed rows\n",
    "fixed_df = pd.DataFrame(fixed_data).reset_index(drop=True)\n",
    "fixed_df.to_excel(output_fixed, index=False)\n",
    "\n",
    "# Remaining problematic rows\n",
    "still_problematic_df = pd.DataFrame(still_problematic)\n",
    "still_problematic_df.to_excel(output_unfixed, index=False)\n",
    "\n",
    "print(f\"✅ Fixed rows saved to: {output_fixed}\")\n",
    "print(f\"⚠️ Still unmatched rows saved to: {output_unfixed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9eb19e7-0822-44aa-9140-0b3fe889ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Expanded problematic rows saved to: F:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\expanded_unfixed_fixed.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Step 1: Define paths ===\n",
    "base_dir = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\"\n",
    "input_unfixed_path = os.path.join(base_dir, \"transformed\", \"still_unfixed_problematic_ids.xlsx\")\n",
    "original_data_path = os.path.join(base_dir, \"merged\", \"final_deduplicated_data.xlsx\")\n",
    "output_expanded = os.path.join(base_dir, \"transformed\", \"expanded_unfixed_fixed.xlsx\")\n",
    "\n",
    "# === Step 2: Load still unmatched IDs and original dataset ===\n",
    "still_unfixed_df = pd.read_excel(input_unfixed_path)\n",
    "unfixed_ids = still_unfixed_df['Citeline Drug ID'].tolist()\n",
    "\n",
    "df_original = pd.read_excel(original_data_path)\n",
    "for col in ['Event Date', 'Event Type', 'Event Details']:\n",
    "    df_original[col] = df_original[col].fillna('').astype(str)\n",
    "\n",
    "# === Step 3: Filter original dataset for those IDs ===\n",
    "df_unfixed = df_original[df_original['Citeline Drug ID'].isin(unfixed_ids)].copy()\n",
    "\n",
    "# === Step 4: Smart split logic ===\n",
    "def smart_split_details(text):\n",
    "    lines = re.split(r'\\r?\\n', text)\n",
    "    merged = []\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            merged.append('')\n",
    "            continue\n",
    "        if merged and line[0].islower() and merged[-1] != '':\n",
    "            merged[-1] += ' ' + line.strip()\n",
    "        else:\n",
    "            merged.append(line.strip())\n",
    "    return merged\n",
    "\n",
    "# === Step 5: Expand each row by max length ===\n",
    "expanded_rows = []\n",
    "\n",
    "for _, row in df_unfixed.iterrows():\n",
    "    dates = re.split(r'\\r?\\n', row['Event Date'])\n",
    "    types = re.split(r'\\r?\\n', row['Event Type'])\n",
    "    details = smart_split_details(row['Event Details'])\n",
    "\n",
    "    max_len = max(len(dates), len(types), len(details))\n",
    "    dates += [''] * (max_len - len(dates))\n",
    "    types += [''] * (max_len - len(types))\n",
    "    details += [''] * (max_len - len(details))\n",
    "\n",
    "    for i in range(max_len):\n",
    "        new_row = row.copy()\n",
    "        new_row['Event Date'] = dates[i]\n",
    "        new_row['Event Type'] = types[i]\n",
    "        new_row['Event Details'] = details[i]\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# === Step 6: Save expanded rows ===\n",
    "expanded_df = pd.DataFrame(expanded_rows).reset_index(drop=True)\n",
    "expanded_df.to_excel(output_expanded, index=False)\n",
    "\n",
    "print(f\"📤 Expanded problematic rows saved to: {output_expanded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9500ea9-fa82-4aa4-9c06-226d030f42fb",
   "metadata": {},
   "source": [
    "## append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e460d7e-7c68-481f-9cf4-56a57b123510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Columns NOT common across all files:\n",
      "- transformed: []\n",
      "- fixed: []\n",
      "- expanded: []\n",
      "🧾 Unique Citeline Drug ID Intersections:\n",
      "- Transformed ∩ Fixed: 0 IDs\n",
      "- Transformed ∩ Expanded: 0 IDs\n",
      "- Fixed ∩ Expanded: 0 IDs\n",
      "Intersecting IDs (transformed & fixed): set()\n",
      "Intersecting IDs (transformed & expanded): set()\n",
      "Intersecting IDs (fixed & expanded): set()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Step 1: Define file paths ===\n",
    "base_dir = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\"\n",
    "files = {\n",
    "    \"transformed\": os.path.join(base_dir, \"transformed_data2.xlsx\"),\n",
    "    \"fixed\": os.path.join(base_dir, \"fixed_problematic_ids.xlsx\"),\n",
    "    \"expanded\": os.path.join(base_dir, \"expanded_unfixed_fixed_man.xlsx\")\n",
    "}\n",
    "\n",
    "# === Step 2: Load the files ===\n",
    "dfs = {name: pd.read_excel(path) for name, path in files.items()}\n",
    "\n",
    "# === Step 3: Compare column names ===\n",
    "all_columns = {name: set(df.columns) for name, df in dfs.items()}\n",
    "common_columns = set.intersection(*all_columns.values())\n",
    "all_columns_union = set.union(*all_columns.values())\n",
    "\n",
    "# Identify non-common columns per dataset\n",
    "column_differences = {\n",
    "    name: sorted(cols - common_columns) for name, cols in all_columns.items()\n",
    "}\n",
    "\n",
    "# Print non-matching columns for review\n",
    "print(\"🧾 Columns NOT common across all files:\")\n",
    "for name, diffs in column_differences.items():\n",
    "    print(f\"- {name}: {diffs}\")\n",
    "\n",
    "# === Step 4: Check unique Citeline Drug ID intersections ===\n",
    "unique_ids = {\n",
    "    name: set(df[\"Citeline Drug ID\"].dropna().unique()) for name, df in dfs.items()\n",
    "}\n",
    "\n",
    "# Compute intersections\n",
    "intersect_transformed_fixed = unique_ids[\"transformed\"] & unique_ids[\"fixed\"]\n",
    "intersect_transformed_expanded = unique_ids[\"transformed\"] & unique_ids[\"expanded\"]\n",
    "intersect_fixed_expanded = unique_ids[\"fixed\"] & unique_ids[\"expanded\"]\n",
    "\n",
    "print(\"🧾 Unique Citeline Drug ID Intersections:\")\n",
    "print(f\"- Transformed ∩ Fixed: {len(intersect_transformed_fixed)} IDs\")\n",
    "print(f\"- Transformed ∩ Expanded: {len(intersect_transformed_expanded)} IDs\")\n",
    "print(f\"- Fixed ∩ Expanded: {len(intersect_fixed_expanded)} IDs\")\n",
    "\n",
    "# Optional: Uncomment to see exact intersecting IDs\n",
    "print(\"Intersecting IDs (transformed & fixed):\", intersect_transformed_fixed)\n",
    "print(\"Intersecting IDs (transformed & expanded):\", intersect_transformed_expanded)\n",
    "print(\"Intersecting IDs (fixed & expanded):\", intersect_fixed_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a26133c-f0bb-47b4-8406-adaaeb13a6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset saved to: F:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\combined_2_cleaned.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Step 1: Define file paths ===\n",
    "base_dir = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\"\n",
    "file_list = [\n",
    "    \"transformed_data2.xlsx\",\n",
    "    \"fixed_problematic_ids.xlsx\",\n",
    "    \"expanded_unfixed_fixed_man.xlsx\"\n",
    "]\n",
    "\n",
    "# === Step 2: Load and combine ===\n",
    "dfs = [pd.read_excel(os.path.join(base_dir, file)) for file in file_list]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# === Step 3: Save result ===\n",
    "output_path = os.path.join(base_dir, \"combined_2_cleaned.xlsx\")\n",
    "combined_df.to_excel(output_path, index=False)\n",
    "print(f\"✅ Combined dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299825e8-c0bb-4bf3-881c-c8f7b29a5450",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04520ab2-1e0c-4d52-8e2f-f7a3b0a9ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed data for Citeline Drug ID 77572 saved to:\n",
      "D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\drug_77572_expanded.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\merged\\final_deduplicated_data.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Replace NaN with empty strings and ensure string type\n",
    "for col in ['Event Date', 'Event Type', 'Event Details']:\n",
    "    df[col] = df[col].fillna('').astype(str)\n",
    "\n",
    "# Filter for specific drug ID\n",
    "drug_id = 77572\n",
    "row = df[df['Citeline Drug ID'] == drug_id]\n",
    "\n",
    "if row.empty:\n",
    "    print(f\"No record found for Citeline Drug ID {drug_id}\")\n",
    "else:\n",
    "    row = row.iloc[0]  # Get the row as a Series\n",
    "\n",
    "    # Split on line breaks robustly (handles \\n or \\r\\n)\n",
    "    event_dates = re.split(r'\\r?\\n', row['Event Date'])\n",
    "    event_types = re.split(r'\\r?\\n', row['Event Type'])\n",
    "    event_details = re.split(r'\\r?\\n', row['Event Details'])\n",
    "\n",
    "    # Determine the maximum length\n",
    "    max_len = max(len(event_dates), len(event_types), len(event_details))\n",
    "\n",
    "    # Pad all lists to the same length with ''\n",
    "    event_dates += [''] * (max_len - len(event_dates))\n",
    "    event_types += [''] * (max_len - len(event_types))\n",
    "    event_details += [''] * (max_len - len(event_details))\n",
    "\n",
    "    # Create new rows\n",
    "    parsed_rows = []\n",
    "    for i in range(max_len):\n",
    "        new_row = row.copy()\n",
    "        new_row['Event Date'] = event_dates[i]\n",
    "        new_row['Event Type'] = event_types[i]\n",
    "        new_row['Event Details'] = event_details[i]\n",
    "        parsed_rows.append(new_row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    parsed_df = pd.DataFrame(parsed_rows).reset_index(drop=True)\n",
    "\n",
    "    # Output path\n",
    "    output_path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\drug_77572_expanded.xlsx\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Save to Excel\n",
    "    parsed_df.to_excel(output_path, index=False)\n",
    "    print(f\"Parsed data for Citeline Drug ID {drug_id} saved to:\\n{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c32f4-0b0c-44e1-843e-78e43f68468f",
   "metadata": {},
   "source": [
    "## splite the old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e1870d2-7d7c-4d40-be24-d87eec9a9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation complete. Data saved to: D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed_data1.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where the files are located\n",
    "directory = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\"\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"20200616_filtered.xlsx\"\n",
    "\n",
    "# Combine directory and file name to create the full file path\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "# Load the data from the Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create a new DataFrame to store the transformed data\n",
    "transformed_data = []\n",
    "\n",
    "# Iterate through each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    keyevents = row.get('keyevents', '')\n",
    "    \n",
    "    # Skip rows where keyevents is NaN or empty\n",
    "    if pd.isna(keyevents) or keyevents.strip() == \"\":\n",
    "        continue\n",
    "    \n",
    "    # Adjusted regex to handle semicolons in event details\n",
    "    pattern = r'keyEventDate:(.*?);(?:\\s*keyEventHistory:(.*?);)?(?:\\s*keyEventDetail:(.*?)(?=\\s*keyEventDate:|\\s*keyEventHistory:|\\s*keyEventDetail:|$))?'\n",
    "    matches = re.findall(pattern, keyevents, re.DOTALL)\n",
    "    \n",
    "    for match in matches:\n",
    "        event_date = match[0].strip() if match[0] else \"\"\n",
    "        event_type = match[1].strip() if match[1] else \"\"  # Can be empty\n",
    "        event_details = match[2].strip() if match[2] else \"\"  # Can be empty\n",
    "        \n",
    "        # Copy the row and replace with extracted event data\n",
    "        new_row = row.copy()\n",
    "        new_row['Event Date'] = event_date\n",
    "        new_row['Event Type'] = event_type\n",
    "        new_row['Event Details'] = event_details  # This will now keep everything correctly\n",
    "        \n",
    "        # Append to transformed data\n",
    "        transformed_data.append(new_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_data)\n",
    "\n",
    "# Reset index\n",
    "transformed_df = transformed_df.reset_index(drop=True)\n",
    "\n",
    "# Define output file path in the desired directory\n",
    "output_file_name = \"transformed_data1.xlsx\"\n",
    "output_file_path = os.path.join(directory, output_file_name)\n",
    "\n",
    "# Save the transformed data to a new Excel file\n",
    "transformed_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Transformation complete. Data saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc45e6-2ac2-4e78-af76-7e8f2977c653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
