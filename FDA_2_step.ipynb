{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a914ca1-f63d-4cb6-9474-d37ac598eb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File loaded successfully.\n",
      "\n",
      "Shape: (256, 25)\n",
      "\n",
      "Columns:\n",
      "['ApplNo', 'ProductNo', 'Form', 'Strength', 'ReferenceDrug', 'DrugName', 'ActiveIngredient', 'ReferenceStandard', 'SubmissionClassCodeID', 'SubmissionType', 'SubmissionNo', 'SubmissionStatus', 'SubmissionStatusDate', 'SubmissionsPublicNotes', 'ReviewPriority', 'ApplType', 'ApplPublicNotes', 'SponsorName', 'ApplicationDocsID', 'ApplicationDocsTypeID', 'ApplicationDocsTitle', 'ApplicationDocsURL', 'ApplicationDocsDate', 'j_submissionActionTypeID', 'ActionTypes_LookupID']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the full path to your Excel file\n",
    "file_path = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\test_keytruda.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "print(\"‚úÖ File loaded successfully.\\n\")\n",
    "\n",
    "# Show the number of rows and columns\n",
    "print(f\"Shape: {df.shape}\")\n",
    "# Show column names\n",
    "print(\"\\nColumns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d152eb5d-4248-4272-9d95-15a44409b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# --- PDF keyword analysis function ---\n",
    "def analyze_fda_pdf_from_url(pdf_url, keywords=None, stop_section=\"APPROVAL & LABELING\"):\n",
    "    if keywords is None:\n",
    "        keywords = [\"new indication\", \"modification\", \"treatment\", \"pmr\", \"regular approval\", \"accelerated\"]\n",
    "    result = {kw: 0 for kw in keywords}\n",
    "    result[\"pdf_opened_successfully\"] = False\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Referer\": \"https://www.accessdata.fda.gov/\"\n",
    "        }\n",
    "        response = requests.get(pdf_url, headers=headers, timeout=20)\n",
    "        if response.status_code != 200:\n",
    "            result[\"error\"] = f\"Failed to download (status code: {response.status_code})\"\n",
    "            return result\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "            tmp_file.write(response.content)\n",
    "            tmp_path = tmp_file.name\n",
    "\n",
    "        doc = fitz.open(tmp_path)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        doc.close()\n",
    "        os.remove(tmp_path)\n",
    "\n",
    "        stop_index = full_text.upper().find(stop_section.upper())\n",
    "        pre_section_text = full_text[:stop_index] if stop_index != -1 else full_text\n",
    "\n",
    "        lower_text = pre_section_text.lower()\n",
    "        for kw in keywords:\n",
    "            result[kw] = lower_text.count(kw)\n",
    "\n",
    "        result[\"pdf_opened_successfully\"] = True\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = str(e)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ad5e2d-d1a5-445e-8ba4-f9f406193071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize result columns ---\n",
    "keywords = [\"new indication\", \"modification\", \"treatment\", \"pmr\", \"regular approval\", \"accelerated\"]\n",
    "keyword_cols = keywords + [\"pdf_opened_successfully\", \"error\"]\n",
    "\n",
    "for col in keyword_cols:\n",
    "    df[col] = None\n",
    "\n",
    "# --- Loop through each row and apply logic conditionally ---\n",
    "for idx, row in df.iterrows():\n",
    "    if row.get(\"ActionTypes_LookupID\") == 8 and row.get(\"ApplicationDocsTypeID\") == 1:\n",
    "        url = row.get(\"ApplicationDocsURL\")\n",
    "        if pd.notnull(url):\n",
    "            result = analyze_fda_pdf_from_url(url)\n",
    "            for key, val in result.items():\n",
    "                df.at[idx, key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faca257b-9160-4d49-9ca3-e96d8b7e782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Retry Attempt 1\n",
      "‚úÖ All PDFs processed successfully.\n",
      "\n",
      "‚úÖ Retry loop completed.\n"
     ]
    }
   ],
   "source": [
    "MAX_RETRIES = 5\n",
    "\n",
    "for attempt in range(1, MAX_RETRIES + 1):\n",
    "    print(f\"\\nüîÅ Retry Attempt {attempt}\")\n",
    "\n",
    "    # Identify rows that failed AND have a non-null URL\n",
    "    failed_rows = df[\n",
    "        (df[\"pdf_opened_successfully\"] == False) &\n",
    "        (df[\"ApplicationDocsURL\"].notnull())\n",
    "    ]\n",
    "\n",
    "    if failed_rows.empty:\n",
    "        print(\"‚úÖ All PDFs processed successfully.\")\n",
    "        break\n",
    "\n",
    "    print(f\"  ‚Üí {len(failed_rows)} rows to retry...\")\n",
    "\n",
    "    for idx, row in failed_rows.iterrows():\n",
    "        url = row[\"ApplicationDocsURL\"]\n",
    "        if pd.notnull(url):\n",
    "            result = analyze_fda_pdf_from_url(url, keywords=keywords)\n",
    "            for key, val in result.items():\n",
    "                df.at[idx, key] = val\n",
    "\n",
    "print(\"\\n‚úÖ Retry loop completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa55ca5-f88e-4f6d-b6b7-289466c7215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All rows preserved. Results saved to: F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\test_keytruda_result.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- Save the full DataFrame with new columns ---\n",
    "output_file = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\test_keytruda_result.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ All rows preserved. Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17eb84-65db-4f34-a4c3-4c2289cd8ffa",
   "metadata": {},
   "source": [
    "# Bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f235fa52-212a-4c53-aea8-757bf20eeb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# File paths\n",
    "input_path = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\merged4_modified.xlsx\"\n",
    "output_path_all = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\merged4_non_missing.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_path)\n",
    "\n",
    "# Filter rows where ProductNo is not missing or blank\n",
    "df_non_missing = df[df[\"ApplicationDocsURL\"].notna() & (df[\"ApplicationDocsURL\"].astype(str).str.strip() != \"\")]\n",
    "\n",
    "# Save all non-missing ProductNo records\n",
    "df_non_missing.to_excel(output_path_all, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37013358-9516-4523-8369-6253591e693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# File paths\n",
    "input_path = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\merged4_non_missing.xlsx\"\n",
    "output_path_1000 = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\merged4_non_missing_top1000.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_path)\n",
    "# Save only the first 1000 rows\n",
    "df.head(20000).to_excel(output_path_1000, index=False)\n",
    "\n",
    "print(\"Export complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbcf2bbd-b846-4449-9919-e63bf622a154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteration 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [14:27<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Failed PDFs = 6\n",
      "\n",
      "--- Iteration 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:22<00:00, 13.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: Failed PDFs = 1\n",
      "\n",
      "--- Iteration 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: Failed PDFs = 0\n",
      "Results saved to F:/PhD/RA/Schafer/IRA/data/unzipped/merge/merged4_non_missing_top1000_improve.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import requests\n",
    "import tempfile\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "main_bullets = [\"‚Ä¢\", \"\\uf0b7\", \"¬∑\", \"\\u00b7\", \".\"]\n",
    "\n",
    "def is_two_column(page, center_width_fraction=0.02, min_text_length=26):\n",
    "    width = page.width\n",
    "    height = page.height\n",
    "    center_start = width * (0.5 - center_width_fraction / 2)\n",
    "    center_end = width * (0.5 + center_width_fraction / 2)\n",
    "    center_text = page.within_bbox((center_start, 0, center_end, height)).extract_text() or \"\"\n",
    "    return len(center_text) <= min_text_length\n",
    "\n",
    "def extract_all_text(pdf_path):\n",
    "    all_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            two_col = is_two_column(page)\n",
    "            if two_col:\n",
    "                width = page.width\n",
    "                left_col = page.within_bbox((0, 0, width / 2, page.height)).extract_text() or \"\"\n",
    "                right_col = page.within_bbox((width / 2, 0, width, page.height)).extract_text() or \"\"\n",
    "                all_text += left_col + \"\\n\" + right_col + \"\\n\"\n",
    "            else:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                all_text += page_text + \"\\n\"\n",
    "    return all_text\n",
    "\n",
    "def find_section_indices(lines, is_2col):\n",
    "    if is_2col:\n",
    "        # LOOSER: anywhere in the line\n",
    "        ind_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"indicat.{0,30}usag\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        dos_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"dosag.{0,20}admin\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        contra_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"contraindicat\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "    else:\n",
    "        # STRICTER: must be at line start (with optional dash/number)\n",
    "        ind_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*indicat.{0,30}usag\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        dos_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*dosag.{0,20}admin\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        contra_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*contraindicat\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "    return ind_start, dos_start, contra_start\n",
    "\n",
    "def get_section_lines_with_gap(ind_idx, dos_indices, lines, gaps=[5, 10, 15]):\n",
    "    for gap in gaps:\n",
    "        valid_dos_indices = [i for i in dos_indices if i - ind_idx > gap]\n",
    "        if valid_dos_indices:\n",
    "            first_valid = valid_dos_indices[0]\n",
    "            return lines[ind_idx+1:first_valid], gap\n",
    "    return [], None\n",
    "\n",
    "def extract_section_loose(lines, is_2col):\n",
    "    ind_start, dos_start, contra_start = find_section_indices(lines, is_2col)\n",
    "    if is_2col:\n",
    "        if ind_start and dos_start:\n",
    "            section_lines, used_gap = get_section_lines_with_gap(ind_start[0], dos_start, lines)\n",
    "            if section_lines:\n",
    "                return \"\\n\".join([lines[ind_start[0]]] + section_lines)\n",
    "            else:\n",
    "                return \"\\n\".join(lines[ind_start[0]:])\n",
    "        elif ind_start:\n",
    "            return \"\\n\".join(lines[ind_start[0]:])\n",
    "        else:\n",
    "            return \"\"\n",
    "    else:\n",
    "        if ind_start and contra_start:\n",
    "            stop = [i for i in contra_start if i > ind_start[0]]\n",
    "            if stop:\n",
    "                stop = stop[0]\n",
    "                return \"\\n\".join(lines[ind_start[0]:stop])\n",
    "        elif ind_start:\n",
    "            return \"\\n\".join(lines[ind_start[0]:])\n",
    "        else:\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def split_by_main_bullet(text):\n",
    "    bullet_chars = ''.join(re.escape(b) for b in main_bullets if b != \".\")\n",
    "    pattern = r'((?:^|\\n)[{}]\\s)'.format(bullet_chars)\n",
    "    pattern += r'|((?:^|\\n)\\.\\s)'\n",
    "    splits = [m.start() for m in re.finditer(pattern, text)]\n",
    "    if not splits:\n",
    "        return [text]\n",
    "    sections = []\n",
    "    for i, idx in enumerate(splits):\n",
    "        end = splits[i+1] if i+1 < len(splits) else len(text)\n",
    "        sections.append(text[idx:end].strip())\n",
    "    return sections\n",
    "\n",
    "def is_sub_bullet_line(line):\n",
    "    line_stripped = line.lstrip()\n",
    "    return line_stripped.startswith(\"o \") or line_stripped.startswith(\"\\u25cb\")\n",
    "\n",
    "def count_sub_bullets(section):\n",
    "    lines = section.splitlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if is_sub_bullet_line(line):\n",
    "            count += 1\n",
    "    return count if count > 0 else 1\n",
    "\n",
    "def should_exclude_section(section):\n",
    "    return 'limitation of use' in section.lower()\n",
    "\n",
    "def count_indicat_and_treatment(section_text):\n",
    "    lines = section_text.splitlines()\n",
    "    count_indicat = max(0, sum(1 for line in lines if re.search(r\"indicated\", line, re.IGNORECASE)) - 1)\n",
    "    count_treatment = sum(1 for line in lines if re.search(r\"treatm\", line, re.IGNORECASE))\n",
    "    return count_indicat, count_treatment\n",
    "\n",
    "def find_max_numbered_bullet(lines):\n",
    "    numbers = []\n",
    "    for line in lines:\n",
    "        match = re.match(r\"^\\s*(\\d+)[\\.\\)]\", line)\n",
    "        if match:\n",
    "            numbers.append(int(match.group(1)))\n",
    "    return max(numbers) if numbers else 0\n",
    "\n",
    "def count_indications_keyword_and_number(section_text):\n",
    "    lines = section_text.splitlines()\n",
    "    count_indicat, count_treatment = count_indicat_and_treatment(section_text)\n",
    "    max_keyword = max(count_indicat, count_treatment)\n",
    "    max_number = find_max_numbered_bullet(lines)\n",
    "    final_count = max(max_keyword, max_number)\n",
    "    return final_count\n",
    "\n",
    "def get_indication_count(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "            tmp_file.write(response.content)\n",
    "            tmp_path = tmp_file.name\n",
    "        with pdfplumber.open(tmp_path) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            is_2col = is_two_column(first_page)\n",
    "            all_text = extract_all_text(tmp_path)\n",
    "            lines = all_text.splitlines()\n",
    "            section = extract_section_loose(lines, is_2col=is_2col)\n",
    "            if is_2col:\n",
    "                main_sections = [\n",
    "                    s for s in split_by_main_bullet(section)\n",
    "                    if s.strip() and not should_exclude_section(s)\n",
    "                ]\n",
    "                count = 0\n",
    "                for sec in main_sections:\n",
    "                    cnt = count_sub_bullets(sec)\n",
    "                    count += cnt\n",
    "            else:\n",
    "                count = count_indications_keyword_and_number(section)\n",
    "        return count, True\n",
    "    except Exception as e:\n",
    "        return None, False\n",
    "\n",
    "def run_auto_pdf_extraction(\n",
    "    excel_path,\n",
    "    out_path=\"indication_results.xlsx\",\n",
    "    url_col=\"ApplicationDocsURL\",\n",
    "    max_retries=5\n",
    "):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    mask = (\n",
    "        (df[\"ApplicationDocsTypeID\"] == 2) &\n",
    "        (df[\"ActionTypes_LookupID\"] < 13) &\n",
    "        (df[\"ApplicationDocsURL\"].notnull())\n",
    "    )\n",
    "    df_proc = df.loc[mask].copy()\n",
    "    indication_counts = [None] * len(df_proc)\n",
    "    open_successes = [False] * len(df_proc)\n",
    "    failed_indices = list(range(len(df_proc)))\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        print(f\"\\n--- Iteration {attempt+1} ---\")\n",
    "        still_failed = []\n",
    "        for idx in tqdm(failed_indices):\n",
    "            row = df_proc.iloc[idx]\n",
    "            count, opened = get_indication_count(row[url_col])\n",
    "            indication_counts[idx] = count\n",
    "            open_successes[idx] = opened\n",
    "            if not opened:\n",
    "                still_failed.append(idx)\n",
    "                time.sleep(1)\n",
    "        failed_indices = still_failed\n",
    "        print(f\"Iteration {attempt+1}: Failed PDFs = {len(failed_indices)}\")\n",
    "        if not failed_indices:\n",
    "            break\n",
    "\n",
    "    df_proc[\"indication_count\"] = indication_counts\n",
    "    df_proc[\"pdf_opened\"] = open_successes\n",
    "    df_proc.to_excel(out_path, index=False)\n",
    "    print(f\"Results saved to {out_path}\")\n",
    "    if failed_indices:\n",
    "        print(f\"Still failed indices after {max_retries} iterations: {failed_indices}\")\n",
    "\n",
    "# ---- Usage ----\n",
    "run_auto_pdf_extraction(\n",
    "    excel_path=\"F:/PhD/RA/Schafer/IRA/data/unzipped/merge/merged4_non_missing_top1000.xlsx\",     # <-- your input file path\n",
    "    out_path=\"F:/PhD/RA/Schafer/IRA/data/unzipped/merge/merged4_non_missing_top1000_improve.xlsx\" # <-- your desired output file path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017c03e-5837-4afa-9803-faa22242b525",
   "metadata": {},
   "source": [
    "# for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b1fbf3b-eb1c-465b-a2e1-d5e098e0b775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol: '-', Unicode: U+002D\n",
      "Symbol: '[', Unicode: U+005B\n",
      "Symbol: ',', Unicode: U+002C\n",
      "Symbol: '(', Unicode: U+0028\n",
      "Symbol: ':', Unicode: U+003A\n",
      "Symbol: '%', Unicode: U+0025\n",
      "Symbol: ']', Unicode: U+005D\n",
      "Symbol: '.', Unicode: U+002E\n",
      "Symbol: '‚Ä¢', Unicode: U+2022\n",
      "Symbol: '‚â•', Unicode: U+2265\n",
      "Symbol: '/', Unicode: U+002F\n",
      "Symbol: ')', Unicode: U+0029\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set(section_text)\n",
    "for c in unique_chars:\n",
    "    if not c.isalnum() and not c.isspace():  # likely a symbol\n",
    "        print(f\"Symbol: {repr(c)}, Unicode: U+{ord(c):04X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802eeeb-e6f7-4660-9124-8167b10957c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"http://www.accessdata.fda.gov/drugsatfda_docs/label/2006/103705s5230-s5231lbl.pdf\"\n",
    "\n",
    "# Define bullet characters\n",
    "possible_bullets = [\"‚Ä¢\", \"\\uf0b7\", \"¬∑\", \"\\u00b7\"]\n",
    "\n",
    "# Download PDF to a temporary file\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\", \"Referer\": \"https://www.accessdata.fda.gov/\"}\n",
    "response = requests.get(pdf_url, headers=headers, timeout=30)\n",
    "response.raise_for_status()\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "    tmp_file.write(response.content)\n",
    "    tmp_path = tmp_file.name\n",
    "all_text = \"\"\n",
    "with pdfplumber.open(tmp_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        print(is_two_column(page))\n",
    "os.remove(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26de6b63-bd62-4524-a1a7-c85113d6da8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 is_two_column: True\n",
      "Page 2 is_two_column: False\n",
      "Page 3 is_two_column: False\n",
      "Page 4 is_two_column: False\n",
      "Page 5 is_two_column: False\n",
      "Page 6 is_two_column: False\n",
      "Page 7 is_two_column: False\n",
      "Page 8 is_two_column: False\n",
      "Page 9 is_two_column: False\n",
      "Page 10 is_two_column: False\n",
      "Page 11 is_two_column: False\n",
      "Page 12 is_two_column: False\n",
      "Page 13 is_two_column: False\n",
      "Page 14 is_two_column: False\n",
      "Page 15 is_two_column: False\n",
      "Page 16 is_two_column: False\n",
      "Page 17 is_two_column: False\n",
      "Page 18 is_two_column: False\n",
      "Page 19 is_two_column: False\n",
      "Page 20 is_two_column: False\n",
      "Page 21 is_two_column: False\n",
      "Page 22 is_two_column: False\n",
      "Page 23 is_two_column: False\n",
      "Page 24 is_two_column: True\n",
      "Page 25 is_two_column: False\n",
      "[ERROR] No dashed 'INDICATIONS AND USAGE' header found.\n",
      "[INFO] Section extraction returned empty. See error(s) above.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import requests\n",
    "import tempfile\n",
    "import re\n",
    "\n",
    "main_bullets = [\"‚Ä¢\", \"\\uf0b7\", \"¬∑\", \"\\u00b7\", \".\"]\n",
    "\n",
    "def is_two_column(page, center_width_fraction=0.02, min_text_length=26):\n",
    "    width = page.width\n",
    "    height = page.height\n",
    "    center_start = width * (0.5 - center_width_fraction / 2)\n",
    "    center_end = width * (0.5 + center_width_fraction / 2)\n",
    "    center_text = page.within_bbox((center_start, 0, center_end, height)).extract_text() or \"\"\n",
    "    return len(center_text) <= min_text_length\n",
    "\n",
    "def extract_all_text(pdf_path):\n",
    "    all_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            two_col = is_two_column(page)\n",
    "            print(f\"Page {page_num+1} is_two_column: {two_col}\")\n",
    "            if two_col:\n",
    "                width = page.width\n",
    "                left_col = page.within_bbox((0, 0, width / 2, page.height)).extract_text() or \"\"\n",
    "                right_col = page.within_bbox((width / 2, 0, width, page.height)).extract_text() or \"\"\n",
    "                all_text += left_col + \"\\n\" + right_col + \"\\n\"\n",
    "            else:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                all_text += page_text + \"\\n\"\n",
    "    return all_text\n",
    "\n",
    "def find_loose_section_line(lines, pattern):\n",
    "    pat = re.compile(pattern, re.IGNORECASE)\n",
    "    return [i for i, line in enumerate(lines) if pat.search(line.replace(\" \", \"\"))]\n",
    "\n",
    "def find_loose_and_dashed_section_index(lines, pattern):\n",
    "    indices = find_loose_section_line(lines, pattern)\n",
    "    for idx in indices:\n",
    "        if \"--\" in lines[idx]:\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "def find_next_dashed_section_index(lines, pattern, after_idx):\n",
    "    pat = re.compile(pattern, re.IGNORECASE)\n",
    "    for i in range(after_idx+1, len(lines)):\n",
    "        if pat.search(lines[i].replace(\" \", \"\")) and \"--\" in lines[i]:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def find_section_indices(lines, is_2col):\n",
    "    if is_2col:\n",
    "        # Loose match (anywhere in line)\n",
    "        ind_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"indicat.{0,30}usag\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        dos_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"dosag.{0,20}admin\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        contra_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"contraindicat\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "    else:\n",
    "        # Strict match (line start, optional dash/number)\n",
    "        ind_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*indicat.{0,30}usag\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        dos_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*dosag.{0,20}admin\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        contra_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*contraindicat\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "    return ind_start, dos_start, contra_start\n",
    "\n",
    "def get_section_lines_with_gap(ind_idx, dos_indices, lines, gaps=[5, 10, 15]):\n",
    "    for gap in gaps:\n",
    "        valid_dos_indices = [i for i in dos_indices if i - ind_idx > gap]\n",
    "        if valid_dos_indices:\n",
    "            first_valid = valid_dos_indices[0]\n",
    "            return lines[ind_idx+1:first_valid], gap\n",
    "    return [], None\n",
    "\n",
    "def extract_section_loose(lines, is_2col):\n",
    "    if is_2col:\n",
    "        ind_pattern = r\"indicat.{0,30}usag\"\n",
    "        dos_pattern = r\"dosag.{0,20}admin\"\n",
    "\n",
    "        start_idx = find_loose_and_dashed_section_index(lines, ind_pattern)\n",
    "        if start_idx is None:\n",
    "            print(\"[ERROR] No dashed 'INDICATIONS AND USAGE' header found.\")\n",
    "            return \"\"\n",
    "        end_idx = find_next_dashed_section_index(lines, dos_pattern, start_idx)\n",
    "        if end_idx is None:\n",
    "            print(\"[ERROR] No dashed 'DOSAGE AND ADMINISTRATION' header found after 'INDICATIONS AND USAGE'.\")\n",
    "            return \"\"\n",
    "        return \"\\n\".join(lines[start_idx+1:end_idx])\n",
    "    else:\n",
    "        ind_start, _, contra_start = find_section_indices(lines, is_2col)\n",
    "        if ind_start and contra_start:\n",
    "            stop = [i for i in contra_start if i > ind_start[0]]\n",
    "            if stop:\n",
    "                stop = stop[0]\n",
    "                return \"\\n\".join(lines[ind_start[0]:stop])\n",
    "        elif ind_start:\n",
    "            return \"\\n\".join(lines[ind_start[0]:])\n",
    "        else:\n",
    "            print(\"[ERROR] No 'INDICATIONS AND USAGE' header found in one-column section.\")\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def split_by_main_bullet(text):\n",
    "    bullet_chars = ''.join(re.escape(b) for b in main_bullets if b != \".\")\n",
    "    pattern = r'((?:^|\\n)[{}]\\s)'.format(bullet_chars)\n",
    "    pattern += r'|((?:^|\\n)\\.\\s)'\n",
    "    splits = [m.start() for m in re.finditer(pattern, text)]\n",
    "    if not splits:\n",
    "        return [text]\n",
    "    sections = []\n",
    "    for i, idx in enumerate(splits):\n",
    "        end = splits[i+1] if i+1 < len(splits) else len(text)\n",
    "        sections.append(text[idx:end].strip())\n",
    "    return sections\n",
    "\n",
    "def is_sub_bullet_line(line):\n",
    "    line_stripped = line.lstrip()\n",
    "    return line_stripped.startswith(\"o \") or line_stripped.startswith(\"\\u25cb\")\n",
    "\n",
    "def count_sub_bullets(section):\n",
    "    lines = section.splitlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if is_sub_bullet_line(line):\n",
    "            count += 1\n",
    "    return count if count > 0 else 1\n",
    "\n",
    "def should_exclude_section(section):\n",
    "    return 'limitation of use' in section.lower()\n",
    "\n",
    "def count_indicat_and_treatment(section_text):\n",
    "    lines = section_text.splitlines()\n",
    "    count_indicat = max(0, sum(1 for line in lines if re.search(r\"indicated\", line, re.IGNORECASE)) - 1)\n",
    "    count_treatment = sum(1 for line in lines if re.search(r\"treatm\", line, re.IGNORECASE))\n",
    "    return count_indicat, count_treatment\n",
    "\n",
    "def find_max_numbered_bullet(lines):\n",
    "    numbers = []\n",
    "    for line in lines:\n",
    "        match = re.match(r\"^\\s*(\\d+)[\\.\\)]\", line)\n",
    "        if match:\n",
    "            numbers.append(int(match.group(1)))\n",
    "    return max(numbers) if numbers else 0\n",
    "\n",
    "def count_indications_keyword_and_number(section_text):\n",
    "    lines = section_text.splitlines()\n",
    "    count_indicat, count_treatment = count_indicat_and_treatment(section_text)\n",
    "    max_keyword = max(count_indicat, count_treatment)\n",
    "    max_number = find_max_numbered_bullet(lines)\n",
    "    final_count = max(max_keyword, max_number)\n",
    "    return final_count\n",
    "\n",
    "def test_pdf_indication_count(pdf_url):\n",
    "    response = requests.get(pdf_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "        tmp_file.write(response.content)\n",
    "        tmp_path = tmp_file.name\n",
    "    with pdfplumber.open(tmp_path) as pdf:\n",
    "        first_page = pdf.pages[0]\n",
    "        is_2col = is_two_column(first_page)\n",
    "        all_text = extract_all_text(tmp_path)\n",
    "        lines = all_text.splitlines()\n",
    "        section = extract_section_loose(lines, is_2col=is_2col)\n",
    "\n",
    "        if not section.strip():\n",
    "            print(\"[INFO] Section extraction returned empty. See error(s) above.\")\n",
    "            return\n",
    "\n",
    "        if is_2col:\n",
    "            main_sections = [\n",
    "                s for s in split_by_main_bullet(section)\n",
    "                if s.strip() and not should_exclude_section(s)\n",
    "            ]\n",
    "            count = 0\n",
    "            for sec in main_sections:\n",
    "                cnt = count_sub_bullets(sec)\n",
    "                count += cnt\n",
    "            method = \"two-column (strict dashed header logic only)\"\n",
    "        else:\n",
    "            count = count_indications_keyword_and_number(section)\n",
    "            method = \"one-column (max of keyword match or numbered bullet, strict header)\"\n",
    "\n",
    "        preview = \"\\n\".join(section.splitlines()[:25])\n",
    "        print(f\"\\n--- Section Preview (first 25 lines) ---\\n{preview}\")\n",
    "        print(f\"\\n--- Total lines in section: {len(section.splitlines())}\")\n",
    "        print(f\"\\nIndication count: {count} (using {method})\")\n",
    "\n",
    "# Example usage:\n",
    "test_pdf_indication_count(\"http://www.accessdata.fda.gov/drugsatfda_docs/label/2020/008085Orig1s071lbl.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf49caa9-349f-4d74-9e03-7bb067682843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 is_two_column: False\n",
      "Page 2 is_two_column: True\n",
      "Page 3 is_two_column: False\n",
      "Page 4 is_two_column: False\n",
      "Page 5 is_two_column: False\n",
      "Page 6 is_two_column: False\n",
      "Page 7 is_two_column: True\n",
      "Page 8 is_two_column: False\n",
      "Page 9 is_two_column: False\n",
      "Page 10 is_two_column: True\n",
      "Page 11 is_two_column: False\n",
      "Page 12 is_two_column: False\n",
      "Page 13 is_two_column: True\n",
      "Page 14 is_two_column: False\n",
      "Page 15 is_two_column: False\n",
      "Page 16 is_two_column: False\n",
      "Page 17 is_two_column: False\n",
      "Page 18 is_two_column: True\n",
      "[ERROR] No 'INDICATIONS AND USAGE' header found in one-column section.\n",
      "[INFO] Section extraction returned empty. See error(s) above.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import requests\n",
    "import tempfile\n",
    "import re\n",
    "\n",
    "main_bullets = [\"‚Ä¢\", \"\\uf0b7\", \"¬∑\", \"\\u00b7\", \".\"]\n",
    "\n",
    "def is_two_column(page, center_width_fraction=0.02, min_text_length=26):\n",
    "    width = page.width\n",
    "    height = page.height\n",
    "    center_start = width * (0.5 - center_width_fraction / 2)\n",
    "    center_end = width * (0.5 + center_width_fraction / 2)\n",
    "    center_text = page.within_bbox((center_start, 0, center_end, height)).extract_text() or \"\"\n",
    "    return len(center_text) <= min_text_length\n",
    "\n",
    "def extract_all_text(pdf_path):\n",
    "    all_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            two_col = is_two_column(page)\n",
    "            print(f\"Page {page_num+1} is_two_column: {two_col}\")\n",
    "            if two_col:\n",
    "                width = page.width\n",
    "                left_col = page.within_bbox((0, 0, width / 2, page.height)).extract_text() or \"\"\n",
    "                right_col = page.within_bbox((width / 2, 0, width, page.height)).extract_text() or \"\"\n",
    "                all_text += left_col + \"\\n\" + right_col + \"\\n\"\n",
    "            else:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                all_text += page_text + \"\\n\"\n",
    "    return all_text\n",
    "\n",
    "def find_loose_and_dashed_section_index(lines, pattern):\n",
    "    # Match lines with the pattern and at least 2 dashes or underscores\n",
    "    pat = re.compile(pattern, re.IGNORECASE)\n",
    "    for idx, line in enumerate(lines):\n",
    "        if pat.search(line.replace(\" \", \"\")) and (\n",
    "            \"--\" in line or \"__\" in line\n",
    "        ):\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "def find_next_dashed_section_index(lines, pattern, after_idx):\n",
    "    # Find next line after after_idx matching pattern and with dashes or underscores\n",
    "    pat = re.compile(pattern, re.IGNORECASE)\n",
    "    for i in range(after_idx + 1, len(lines)):\n",
    "        if pat.search(lines[i].replace(\" \", \"\")) and (\n",
    "            \"--\" in lines[i] or \"__\" in lines[i]\n",
    "        ):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def find_section_indices(lines, is_2col):\n",
    "    if is_2col:\n",
    "        # Loose match (anywhere in line)\n",
    "        ind_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"indicat.{0,30}usag\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        dos_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"dosag.{0,20}admin\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        contra_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"contraindicat\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "    else:\n",
    "        # Strict match (line start, optional dash/number)\n",
    "        ind_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*indicat.{0,30}usag\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        dos_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*dosag.{0,20}admin\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        contra_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*contraindicat\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "    return ind_start, dos_start, contra_start\n",
    "\n",
    "def get_section_lines_with_gap(ind_idx, dos_indices, lines, gaps=[5, 10, 15]):\n",
    "    for gap in gaps:\n",
    "        valid_dos_indices = [i for i in dos_indices if i - ind_idx > gap]\n",
    "        if valid_dos_indices:\n",
    "            first_valid = valid_dos_indices[0]\n",
    "            return lines[ind_idx+1:first_valid], gap\n",
    "    return [], None\n",
    "\n",
    "def extract_section_loose(lines, is_2col):\n",
    "    if is_2col:\n",
    "        ind_pattern = r\"indicat.{0,30}usag\"\n",
    "        dos_pattern = r\"dosag.{0,20}admin\"\n",
    "\n",
    "        # 1. Try dashed/underscored header logic\n",
    "        start_idx = find_loose_and_dashed_section_index(lines, ind_pattern)\n",
    "        end_idx = None\n",
    "        if start_idx is not None:\n",
    "            end_idx = find_next_dashed_section_index(lines, dos_pattern, start_idx)\n",
    "        if start_idx is not None and end_idx is not None:\n",
    "            print(\"[INFO] Using dashed/underscored header extraction.\")\n",
    "            return \"\\n\".join(lines[start_idx + 1 : end_idx])\n",
    "        else:\n",
    "            print(\"[INFO] No dashed/underscored section header(s) found, falling back to gap-based extraction.\")\n",
    "\n",
    "        # 2. Fallback: original gap logic with loose detection\n",
    "        ind_start, dos_start, _ = find_section_indices(lines, is_2col)\n",
    "        if ind_start and dos_start:\n",
    "            section_lines, used_gap = get_section_lines_with_gap(ind_start[0], dos_start, lines)\n",
    "            if section_lines:\n",
    "                return \"\\n\".join([lines[ind_start[0]]] + section_lines)\n",
    "            else:\n",
    "                return \"\\n\".join(lines[ind_start[0]:])\n",
    "        elif ind_start:\n",
    "            return \"\\n\".join(lines[ind_start[0]:])\n",
    "        else:\n",
    "            print(\"[ERROR] No 'INDICATIONS AND USAGE' header found at all.\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        # One-column logic (unchanged)\n",
    "        ind_start, _, contra_start = find_section_indices(lines, is_2col)\n",
    "        if ind_start and contra_start:\n",
    "            stop = [i for i in contra_start if i > ind_start[0]]\n",
    "            if stop:\n",
    "                stop = stop[0]\n",
    "                return \"\\n\".join(lines[ind_start[0]:stop])\n",
    "        elif ind_start:\n",
    "            return \"\\n\".join(lines[ind_start[0]:])\n",
    "        else:\n",
    "            print(\"[ERROR] No 'INDICATIONS AND USAGE' header found in one-column section.\")\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def split_by_main_bullet(text):\n",
    "    bullet_chars = ''.join(re.escape(b) for b in main_bullets if b != \".\")\n",
    "    pattern = r'((?:^|\\n)[{}]\\s)'.format(bullet_chars)\n",
    "    pattern += r'|((?:^|\\n)\\.\\s)'\n",
    "    splits = [m.start() for m in re.finditer(pattern, text)]\n",
    "    if not splits:\n",
    "        return [text]\n",
    "    sections = []\n",
    "    for i, idx in enumerate(splits):\n",
    "        end = splits[i+1] if i+1 < len(splits) else len(text)\n",
    "        sections.append(text[idx:end].strip())\n",
    "    return sections\n",
    "\n",
    "def is_sub_bullet_line(line):\n",
    "    line_stripped = line.lstrip()\n",
    "    return line_stripped.startswith(\"o \") or line_stripped.startswith(\"\\u25cb\")\n",
    "\n",
    "def count_sub_bullets(section):\n",
    "    lines = section.splitlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if is_sub_bullet_line(line):\n",
    "            count += 1\n",
    "    return count if count > 0 else 1\n",
    "\n",
    "def should_exclude_section(section):\n",
    "    return 'limitation of use' in section.lower()\n",
    "\n",
    "def count_indicat_and_treatment(section_text):\n",
    "    lines = section_text.splitlines()\n",
    "    count_indicat = max(0, sum(1 for line in lines if re.search(r\"indicated\", line, re.IGNORECASE)))\n",
    "    count_treatment = sum(1 for line in lines if re.search(r\"treatm\", line, re.IGNORECASE))\n",
    "    return count_indicat, count_treatment\n",
    "\n",
    "def find_max_numbered_bullet(lines):\n",
    "    numbers = []\n",
    "    for line in lines:\n",
    "        match = re.match(r\"^\\s*(\\d+)[\\.\\)]\", line)\n",
    "        if match:\n",
    "            numbers.append(int(match.group(1)))\n",
    "    return max(numbers) if numbers else 0\n",
    "\n",
    "def count_indications_keyword_and_number(section_text):\n",
    "    lines = section_text.splitlines()\n",
    "    count_indicat, count_treatment = count_indicat_and_treatment(section_text)\n",
    "    max_keyword = max(count_indicat, count_treatment)\n",
    "    max_number = find_max_numbered_bullet(lines)\n",
    "    final_count = max(max_keyword, max_number)\n",
    "    return final_count\n",
    "\n",
    "def test_pdf_indication_count(pdf_url):\n",
    "    response = requests.get(pdf_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "        tmp_file.write(response.content)\n",
    "        tmp_path = tmp_file.name\n",
    "    with pdfplumber.open(tmp_path) as pdf:\n",
    "        first_page = pdf.pages[0]\n",
    "        is_2col = is_two_column(first_page)\n",
    "        all_text = extract_all_text(tmp_path)\n",
    "        lines = all_text.splitlines()\n",
    "        section = extract_section_loose(lines, is_2col=is_2col)\n",
    "\n",
    "        if not section.strip():\n",
    "            print(\"[INFO] Section extraction returned empty. See error(s) above.\")\n",
    "            return\n",
    "\n",
    "        if is_2col:\n",
    "            main_sections = [\n",
    "                s for s in split_by_main_bullet(section)\n",
    "                if s.strip() and not should_exclude_section(s)\n",
    "            ]\n",
    "            count = 0\n",
    "            for sec in main_sections:\n",
    "                cnt = count_sub_bullets(sec)\n",
    "                count += cnt\n",
    "            method = \"two-column (hybrid: dashed/underscored headers preferred, gap fallback)\"\n",
    "        else:\n",
    "            count = count_indications_keyword_and_number(section)\n",
    "            method = \"one-column (max of keyword match or numbered bullet, strict header)\"\n",
    "\n",
    "        preview = \"\\n\".join(section.splitlines()[:25])\n",
    "        print(f\"\\n--- Section Preview (first 25 lines) ---\\n{preview}\")\n",
    "        print(f\"\\n--- Total lines in section: {len(section.splitlines())}\")\n",
    "        print(f\"\\nIndication count: {count} (using {method})\")\n",
    "\n",
    "# Example usage:\n",
    "test_pdf_indication_count(\"https://www.accessdata.fda.gov/drugsatfda_docs/label/2005/018680s060lbl.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fc471-3f0e-433b-938d-0fae7011d2f0",
   "metadata": {},
   "source": [
    "## Newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6f5dac-34cf-4373-b3dd-58fb786a9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteration 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñä         | 18/208 [00:45<05:31,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] PDF extraction failed: 502 Server Error: Bad Gateway for url: http://www.accessdata.fda.gov/drugsatfda_docs/label/2019/010721s063lbl.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 56/208 [03:30<07:54,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] PDF extraction failed: HTTPSConnectionPool(host='www.accessdata.fda.gov', port=443): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 114/208 [07:44<05:09,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] PDF extraction failed: HTTPSConnectionPool(host='www.accessdata.fda.gov', port=443): Max retries exceeded with url: /drugsatfda_docs/label/2013/018936s100s101,021235s021lbl.pdf (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 132/208 [09:42<04:36,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] PDF extraction failed: HTTPSConnectionPool(host='www.accessdata.fda.gov', port=443): Max retries exceeded with url: /drugsatfda_docs/label/2002/18936s61s65,20101s27,20974s1lbl.pdf (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [14:45<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Failed PDFs = 27\n",
      "\n",
      "--- Iteration 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 7/27 [00:21<01:03,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] PDF extraction failed: HTTPSConnectionPool(host='www.accessdata.fda.gov', port=443): Max retries exceeded with url: /drugsatfda_docs/label/1998/18603s19lbl.pdf (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [01:34<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: Failed PDFs = 23\n",
      "\n",
      "--- Iteration 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 1/23 [00:02<00:45,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] PDF extraction failed: HTTPSConnectionPool(host='www.accessdata.fda.gov', port=443): Max retries exceeded with url: /drugsatfda_docs/label/2023/017031s041lbl.pdf (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [01:09<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: Failed PDFs = 23\n",
      "\n",
      "--- Iteration 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:55<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: Failed PDFs = 23\n",
      "\n",
      "--- Iteration 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:49<00:04,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] PDF extraction failed: HTTPSConnectionPool(host='www.accessdata.fda.gov', port=443): Max retries exceeded with url: /drugsatfda_docs/nda/2000/19304-S005_Tricor_prntlbl.pdf (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [01:12<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: Failed PDFs = 23\n",
      "Results saved to F:/PhD/RA/Schafer/IRA/data/unzipped/merge/merged4_non_missing_top1000_improve.xlsx\n",
      "Still failed indices after 5 iterations: [11, 64, 69, 91, 92, 93, 94, 95, 96, 100, 113, 156, 157, 158, 159, 160, 161, 162, 163, 173, 174, 175, 176]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import requests\n",
    "import tempfile\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "main_bullets = [\"‚Ä¢\", \"\\uf0b7\", \"¬∑\", \"\\u00b7\", \".\"]\n",
    "\n",
    "def is_two_column(page, center_width_fraction=0.02, min_text_length=26):\n",
    "    width = page.width\n",
    "    height = page.height\n",
    "    center_start = width * (0.5 - center_width_fraction / 2)\n",
    "    center_end = width * (0.5 + center_width_fraction / 2)\n",
    "    center_text = page.within_bbox((center_start, 0, center_end, height)).extract_text() or \"\"\n",
    "    return len(center_text) <= min_text_length\n",
    "\n",
    "def extract_all_text(pdf_path):\n",
    "    all_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            two_col = is_two_column(page)\n",
    "            # print(f\"Page {page_num+1} is_two_column: {two_col}\")\n",
    "            if two_col:\n",
    "                width = page.width\n",
    "                left_col = page.within_bbox((0, 0, width / 2, page.height)).extract_text() or \"\"\n",
    "                right_col = page.within_bbox((width / 2, 0, width, page.height)).extract_text() or \"\"\n",
    "                all_text += left_col + \"\\n\" + right_col + \"\\n\"\n",
    "            else:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                all_text += page_text + \"\\n\"\n",
    "    return all_text\n",
    "\n",
    "def find_loose_and_dashed_section_index(lines, pattern):\n",
    "    pat = re.compile(pattern, re.IGNORECASE)\n",
    "    for idx, line in enumerate(lines):\n",
    "        if pat.search(line.replace(\" \", \"\")) and (\n",
    "            \"--\" in line or \"__\" in line\n",
    "        ):\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "def find_next_dashed_section_index(lines, pattern, after_idx):\n",
    "    pat = re.compile(pattern, re.IGNORECASE)\n",
    "    for i in range(after_idx + 1, len(lines)):\n",
    "        if pat.search(lines[i].replace(\" \", \"\")) and (\n",
    "            \"--\" in lines[i] or \"__\" in lines[i]\n",
    "        ):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def find_section_indices(lines, is_2col):\n",
    "    if is_2col:\n",
    "        ind_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"indicat.{0,30}usag\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        dos_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"dosag.{0,20}admin\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        contra_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.search(r\"contraindicat\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "    else:\n",
    "        ind_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*indicat.{0,30}usag\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        dos_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*dosag.{0,20}admin\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "        contra_start = [\n",
    "            i for i, line in enumerate(lines)\n",
    "            if re.match(r\"^\\s*(\\d+[\\.\\)]\\s*)?[-‚Äî‚Äì\\s]*contraindicat\", line.replace(\" \", \"\").lower())\n",
    "        ]\n",
    "    return ind_start, dos_start, contra_start\n",
    "\n",
    "def get_section_lines_with_gap(ind_idx, dos_indices, lines, gaps=[5, 10, 15]):\n",
    "    for gap in gaps:\n",
    "        valid_dos_indices = [i for i in dos_indices if i - ind_idx > gap]\n",
    "        if valid_dos_indices:\n",
    "            first_valid = valid_dos_indices[0]\n",
    "            return lines[ind_idx+1:first_valid], gap\n",
    "    return [], None\n",
    "\n",
    "def extract_section_loose(lines, is_2col):\n",
    "    if is_2col:\n",
    "        ind_pattern = r\"indicat.{0,30}usag\"\n",
    "        dos_pattern = r\"dosag.{0,20}admin\"\n",
    "        # 1. Try dashed/underscored header logic\n",
    "        start_idx = find_loose_and_dashed_section_index(lines, ind_pattern)\n",
    "        end_idx = None\n",
    "        if start_idx is not None:\n",
    "            end_idx = find_next_dashed_section_index(lines, dos_pattern, start_idx)\n",
    "        if start_idx is not None and end_idx is not None:\n",
    "            return \"\\n\".join(lines[start_idx + 1 : end_idx])\n",
    "        # 2. Fallback: original gap logic with loose detection\n",
    "        ind_start, dos_start, _ = find_section_indices(lines, is_2col)\n",
    "        if ind_start and dos_start:\n",
    "            section_lines, used_gap = get_section_lines_with_gap(ind_start[0], dos_start, lines)\n",
    "            if section_lines:\n",
    "                return \"\\n\".join([lines[ind_start[0]]] + section_lines)\n",
    "            else:\n",
    "                return \"\\n\".join(lines[ind_start[0]:])\n",
    "        elif ind_start:\n",
    "            return \"\\n\".join(lines[ind_start[0]:])\n",
    "        else:\n",
    "            return \"\"\n",
    "    else:\n",
    "        ind_start, _, contra_start = find_section_indices(lines, is_2col)\n",
    "        if ind_start and contra_start:\n",
    "            stop = [i for i in contra_start if i > ind_start[0]]\n",
    "            if stop:\n",
    "                stop = stop[0]\n",
    "                return \"\\n\".join(lines[ind_start[0]:stop])\n",
    "        elif ind_start:\n",
    "            return \"\\n\".join(lines[ind_start[0]:])\n",
    "        else:\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def split_by_main_bullet(text):\n",
    "    bullet_chars = ''.join(re.escape(b) for b in main_bullets if b != \".\")\n",
    "    pattern = r'((?:^|\\n)[{}]\\s)'.format(bullet_chars)\n",
    "    pattern += r'|((?:^|\\n)\\.\\s)'\n",
    "    splits = [m.start() for m in re.finditer(pattern, text)]\n",
    "    if not splits:\n",
    "        return [text]\n",
    "    sections = []\n",
    "    for i, idx in enumerate(splits):\n",
    "        end = splits[i+1] if i+1 < len(splits) else len(text)\n",
    "        sections.append(text[idx:end].strip())\n",
    "    return sections\n",
    "\n",
    "def is_sub_bullet_line(line):\n",
    "    line_stripped = line.lstrip()\n",
    "    return line_stripped.startswith(\"o \") or line_stripped.startswith(\"\\u25cb\")\n",
    "\n",
    "def count_sub_bullets(section):\n",
    "    lines = section.splitlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if is_sub_bullet_line(line):\n",
    "            count += 1\n",
    "    return count if count > 0 else 1\n",
    "\n",
    "def should_exclude_section(section):\n",
    "    return 'limitation of use' in section.lower()\n",
    "\n",
    "def count_indicat_and_treatment(section_text):\n",
    "    lines = section_text.splitlines()\n",
    "    count_indicat = max(0, sum(1 for line in lines if re.search(r\"indicated\", line, re.IGNORECASE)))\n",
    "    count_treatment = sum(1 for line in lines if re.search(r\"treatm\", line, re.IGNORECASE))\n",
    "    return count_indicat, count_treatment\n",
    "\n",
    "def find_max_numbered_bullet(lines):\n",
    "    numbers = []\n",
    "    for line in lines:\n",
    "        match = re.match(r\"^\\s*(\\d+)[\\.\\)]\", line)\n",
    "        if match:\n",
    "            numbers.append(int(match.group(1)))\n",
    "    return max(numbers) if numbers else 0\n",
    "\n",
    "def count_indications_keyword_and_number(section_text):\n",
    "    lines = section_text.splitlines()\n",
    "    count_indicat, count_treatment = count_indicat_and_treatment(section_text)\n",
    "    max_keyword = max(count_indicat, count_treatment)\n",
    "    max_number = find_max_numbered_bullet(lines)\n",
    "    final_count = max(max_keyword, max_number)\n",
    "    return final_count\n",
    "\n",
    "def get_indication_count(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "            tmp_file.write(response.content)\n",
    "            tmp_path = tmp_file.name\n",
    "        with pdfplumber.open(tmp_path) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            is_2col = is_two_column(first_page)\n",
    "            all_text = extract_all_text(tmp_path)\n",
    "            lines = all_text.splitlines()\n",
    "            section = extract_section_loose(lines, is_2col=is_2col)\n",
    "            if not section.strip():\n",
    "                return None, False\n",
    "            if is_2col:\n",
    "                main_sections = [\n",
    "                    s for s in split_by_main_bullet(section)\n",
    "                    if s.strip() and not should_exclude_section(s)\n",
    "                ]\n",
    "                count = 0\n",
    "                for sec in main_sections:\n",
    "                    cnt = count_sub_bullets(sec)\n",
    "                    count += cnt\n",
    "            else:\n",
    "                count = count_indications_keyword_and_number(section)\n",
    "            return count, True\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] PDF extraction failed: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def run_auto_pdf_extraction(\n",
    "    excel_path,\n",
    "    out_path=\"indication_results.xlsx\",\n",
    "    url_col=\"ApplicationDocsURL\",\n",
    "    max_retries=5\n",
    "):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    # --- Adjust mask as needed for your data ---\n",
    "    mask = (\n",
    "        (df[\"ApplicationDocsTypeID\"] == 2) &\n",
    "        (df[\"ActionTypes_LookupID\"] < 13) &\n",
    "        (df[\"ApplicationDocsURL\"].notnull())\n",
    "    )\n",
    "    df_proc = df.loc[mask].copy()\n",
    "    indication_counts = [None] * len(df_proc)\n",
    "    open_successes = [False] * len(df_proc)\n",
    "    failed_indices = list(range(len(df_proc)))\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        print(f\"\\n--- Iteration {attempt+1} ---\")\n",
    "        still_failed = []\n",
    "        for idx in tqdm(failed_indices):\n",
    "            row = df_proc.iloc[idx]\n",
    "            count, opened = get_indication_count(row[url_col])\n",
    "            indication_counts[idx] = count\n",
    "            open_successes[idx] = opened\n",
    "            if not opened:\n",
    "                still_failed.append(idx)\n",
    "                time.sleep(1)\n",
    "        failed_indices = still_failed\n",
    "        print(f\"Iteration {attempt+1}: Failed PDFs = {len(failed_indices)}\")\n",
    "        if not failed_indices:\n",
    "            break\n",
    "\n",
    "    df_proc[\"indication_count\"] = indication_counts\n",
    "    df_proc[\"pdf_opened\"] = open_successes\n",
    "    df_proc.to_excel(out_path, index=False)\n",
    "    print(f\"Results saved to {out_path}\")\n",
    "    if failed_indices:\n",
    "        print(f\"Still failed indices after {max_retries} iterations: {failed_indices}\")\n",
    "\n",
    "# ---- Usage Example ----\n",
    "run_auto_pdf_extraction(\n",
    "    excel_path=\"F:/PhD/RA/Schafer/IRA/data/unzipped/merge/merged4_non_missing_top1000.xlsx\",     # <-- input file path\n",
    "    out_path=\"F:/PhD/RA/Schafer/IRA/data/unzipped/merge/merged4_non_missing_top1000_improve.xlsx\" # <-- output file path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd347056-668e-4261-b4c4-4c070eb7b403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ApplNo ProductNo  FirstSubmissionStatusYear  FirstApplicationDocsYear\n",
      "0  10010         1                        NaN                       NaN\n",
      "1  10010         2                        NaN                       NaN\n",
      "2  10012         1                     1981.0                       NaN\n",
      "3  10021        10                     1961.0                    2007.0\n",
      "4  10021         2                     1961.0                    2007.0\n",
      "5  10021         4                     1961.0                    2007.0\n",
      "6  10021         7                     1961.0                    2007.0\n",
      "7  10028         4                     1955.0                       NaN\n",
      "8  10028         5                     1955.0                       NaN\n",
      "9  10040         1                     1955.0                    2002.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# File paths\n",
    "input_path = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\merged4_modified.xlsx\"\n",
    "output_path = r\"F:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\processed\\appl_product_first_years.xlsx\"\n",
    "\n",
    "# --- Load ---\n",
    "df = pd.read_excel(input_path, dtype={\"ApplNo\": str, \"ProductNo\": str})\n",
    "\n",
    "# --- Parse dates safely ---\n",
    "df[\"SubmissionStatusDate\"] = pd.to_datetime(df[\"SubmissionStatusDate\"], errors=\"coerce\")\n",
    "df[\"ApplicationDocsDate\"]  = pd.to_datetime(df[\"ApplicationDocsDate\"], errors=\"coerce\")\n",
    "\n",
    "# --- Sort so earliest SubmissionStatusDate comes first ---\n",
    "df = df.sort_values([\"ApplNo\", \"ProductNo\", \"SubmissionStatusDate\"])\n",
    "\n",
    "# --- Pick the earliest row for each (ApplNo, ProductNo) ---\n",
    "first_rows = df.groupby([\"ApplNo\", \"ProductNo\"], as_index=False).first()\n",
    "\n",
    "# --- Extract years ---\n",
    "first_rows[\"FirstSubmissionStatusYear\"] = first_rows[\"SubmissionStatusDate\"].dt.year\n",
    "first_rows[\"FirstApplicationDocsYear\"]  = first_rows[\"ApplicationDocsDate\"].dt.year\n",
    "\n",
    "# --- Keep only 4 columns ---\n",
    "out = first_rows[[\"ApplNo\", \"ProductNo\", \"FirstSubmissionStatusYear\", \"FirstApplicationDocsYear\"]]\n",
    "\n",
    "# --- Save ---\n",
    "out.to_excel(output_path, index=False)\n",
    "\n",
    "print(out.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d714ca-a277-4763-9da5-667e808249c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
