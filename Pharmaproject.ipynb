{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f508029-43e3-496a-a771-6d7204742239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "path = r'D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\all_complete.xlsx'\n",
    "output_path = r'D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_count.xlsx'\n",
    "\n",
    "df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705ebf62-4cb7-48b5-88eb-2151235ff06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    df['eventtype'].astype(str).str.contains('Approval', case=False, na=False)\n",
    "    & df['eventdetails'].astype(str).str.contains(r'\\bThe US\\b', case=False, na=False)\n",
    ")\n",
    "\n",
    "def count_by_semicolon(s: str) -> int:\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # strip trailing ; and spaces\n",
    "    s = s.rstrip('; ').strip()\n",
    "    # count semicolons -> number of items = semicolons + 1\n",
    "    return s.count(';')-1\n",
    "\n",
    "df['indication_count'] = 0\n",
    "df.loc[mask, 'indication_count'] = df.loc[mask, 'eventdetails'].apply(count_by_semicolon)\n",
    "\n",
    "df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5707d7a6-9508-47d1-bacc-fe675619c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- PASS 1 (priority: new_small/new_bio) ----\n",
      "Exact matches: 934\n",
      "Simple matches (score>=79 & max shared tokens): 1,234\n",
      "---- PASS 2 (general leftovers) ----\n",
      "Exact matches: 85\n",
      "Simple matches (score>=79 & max shared tokens): 944\n",
      "---- PASS 3 (token containment, UNMATCHED indications) ----\n",
      "Token-contained matches: 1,098\n",
      "-------------------------------------\n",
      "Unmatched Part D rows: 409\n",
      "Mapped unique drugids: 1,411\n",
      "Final panel written to: D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_with_partd_totals_by_drugid.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------- PATHS ----------------\n",
    "path = r'D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_count.xlsx'   # must have: drugid, drugprimaryname, origin, nce\n",
    "partd_path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\processed\\partd_combined.xlsx\"\n",
    "\n",
    "out_matches = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\processed\\partd_matches_with_drugid.xlsx\"\n",
    "out_spend_by_drugid = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\unzipped\\merge\\processed\\partd_spend_by_drugid.xlsx\"\n",
    "out_panel   = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_with_partd_totals_by_drugid.xlsx\"\n",
    "\n",
    "for p in [out_matches, out_spend_by_drugid, out_panel]:\n",
    "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "def normalize(s):\n",
    "    \"\"\"Keep + and / so we can tokenize combos; collapse other punctuation to spaces.\"\"\"\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).lower().strip().replace(\"\\u00a0\", \" \")\n",
    "    s = re.sub(r\"[-_,;:()]+\", \" \", s)  # DO NOT remove + or /\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize_for_count(s: str):\n",
    "    # split on +, /, space, hyphen; keep non-empty lower tokens\n",
    "    return [t for t in re.split(r\"[+/ \\-]+\", s.lower().strip()) if t]\n",
    "\n",
    "def tokset(s: str):\n",
    "    return set(tokenize_for_count(s))\n",
    "\n",
    "def token_count(s: str) -> int:\n",
    "    return len(tokset(s))\n",
    "\n",
    "# ---------------- LOAD ----------------\n",
    "df = pd.read_excel(path)          # must have: 'drugid','drugprimaryname','origin','nce'\n",
    "partd = pd.read_excel(partd_path) # must contain: Gnrc_Name (and Tot_Spndng_*)\n",
    "\n",
    "# ----- flags: new_bio, new_small -----\n",
    "origin_str = df[\"origin\"].astype(str).str.lower()\n",
    "df[\"new_bio\"] = (\n",
    "    origin_str.str.contains(\"protein\", na=False) &\n",
    "    (~origin_str.str.contains(r\"bio\\s*-?\\s*similar\", regex=True, na=False))\n",
    ").astype(int)\n",
    "\n",
    "nce_numeric = pd.to_numeric(df[\"nce\"], errors=\"coerce\")\n",
    "df[\"new_small\"] = ((nce_numeric == 1) & (df[\"new_bio\"] == 0)).astype(int)\n",
    "\n",
    "# ---------------- PREP (global) ----------------\n",
    "df = df.copy()\n",
    "df[\"primary_clean\"]  = df[\"drugprimaryname\"].map(normalize)\n",
    "df[\"primary_tok_ct\"] = df[\"primary_clean\"].map(token_count)\n",
    "df[\"primary_tokset\"] = df[\"primary_clean\"].map(tokset)\n",
    "primary_to_name = dict(zip(df[\"primary_clean\"], df[\"drugprimaryname\"]))\n",
    "\n",
    "partd = partd.copy().reset_index(drop=True)\n",
    "partd[\"partd_row_id\"]    = partd.index\n",
    "partd[\"Gnrc_Name_clean\"] = partd[\"Gnrc_Name\"].map(normalize)\n",
    "partd[\"gnrc_tok_ct\"]     = partd[\"Gnrc_Name_clean\"].map(token_count)\n",
    "partd[\"gnrc_tokset\"]     = partd[\"Gnrc_Name_clean\"].map(tokset)\n",
    "\n",
    "# spend columns\n",
    "spend_cols = [c for c in partd.columns if str(c).startswith(\"Tot_Spndng_\")]\n",
    "if spend_cols:\n",
    "    partd[spend_cols] = (\n",
    "        partd[spend_cols]\n",
    "        .apply(lambda s: s.astype(str)\n",
    "               .str.replace(r\"[,\\$]\", \"\", regex=True)\n",
    "               .str.replace(r\"[—–]+\", \"\", regex=True)\n",
    "               .str.replace(r\"(?i)suppressed|n/?a|\\*|nan|none\", \"\", regex=True)\n",
    "               .str.strip())\n",
    "        .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    )\n",
    "\n",
    "# ---------------- MATCHING CORE (no token-count gating) ----------------\n",
    "SIMPLE_CUTOFF = 79   # fuzzy cutoff\n",
    "\n",
    "def run_pass(partd_remaining: pd.DataFrame, df_pool: pd.DataFrame, pass_tag: str):\n",
    "    \"\"\"\n",
    "    Exact match, then fuzzy WITHOUT token-count restrictions:\n",
    "      - Candidates share ≥1 token with the Part D name\n",
    "      - Choose by MAX shared tokens; tie-break by fuzzy score\n",
    "    Returns only the hits from this pass.\n",
    "    \"\"\"\n",
    "    if df_pool.empty or partd_remaining.empty:\n",
    "        return pd.DataFrame(columns=[\"partd_row_id\",\"Gnrc_Name\",\"Gnrc_Name_clean\",\n",
    "                                     \"matched_primary_clean\",\"match_type\",\"match_score\"] + spend_cols)\n",
    "\n",
    "    # Build token index for this pool\n",
    "    primary_set_map  = dict(zip(df_pool[\"primary_clean\"], df_pool[\"primary_tokset\"]))\n",
    "    token_to_primary = defaultdict(set)\n",
    "    for pc, pset in primary_set_map.items():\n",
    "        for t in pset:\n",
    "            token_to_primary[t].add(pc)\n",
    "\n",
    "    # 1) exact within pool\n",
    "    exact = partd_remaining.merge(\n",
    "        df_pool[[\"primary_clean\"]].drop_duplicates(),\n",
    "        left_on=\"Gnrc_Name_clean\",\n",
    "        right_on=\"primary_clean\",\n",
    "        how=\"left\"\n",
    "    ).rename(columns={\"primary_clean\":\"matched_primary_clean\"})\n",
    "\n",
    "    exact[\"match_type\"]  = exact[\"matched_primary_clean\"].notna().map({True: \"generic_exact\", False: None})\n",
    "    exact[\"match_score\"] = exact[\"matched_primary_clean\"].notna().map({True: 100, False: None})\n",
    "\n",
    "    exact_hits   = exact[exact[\"matched_primary_clean\"].notna()].copy()\n",
    "    matched_exact_ids = set(exact_hits[\"partd_row_id\"])\n",
    "\n",
    "    # 2) fuzzy for the remaining Part D rows — NO token-count tolerance, only token overlap\n",
    "    rem = partd_remaining.loc[~partd_remaining[\"partd_row_id\"].isin(matched_exact_ids),\n",
    "                              [\"partd_row_id\",\"Gnrc_Name\",\"Gnrc_Name_clean\",\"gnrc_tok_ct\",\"gnrc_tokset\"] + spend_cols].copy()\n",
    "\n",
    "    rows = []\n",
    "    for _, r in rem.iterrows():\n",
    "        gset = set(r[\"gnrc_tokset\"])\n",
    "        if not gset:\n",
    "            continue\n",
    "\n",
    "        # candidates = any primary sharing ≥1 token\n",
    "        cand_primaries = set()\n",
    "        for t in gset:\n",
    "            cand_primaries |= token_to_primary.get(t, set())\n",
    "        if not cand_primaries:\n",
    "            continue\n",
    "\n",
    "        best_pc, best_shared, best_score = None, -1, 0\n",
    "        for pc in cand_primaries:\n",
    "            pset = primary_set_map[pc]\n",
    "            shared_ct = len(gset & pset)\n",
    "            sc = max(\n",
    "                fuzz.token_set_ratio(r[\"Gnrc_Name_clean\"], pc),\n",
    "                fuzz.token_sort_ratio(r[\"Gnrc_Name_clean\"], pc)\n",
    "            )\n",
    "            if sc < SIMPLE_CUTOFF:\n",
    "                continue\n",
    "            if (shared_ct > best_shared) or (shared_ct == best_shared and sc > best_score):\n",
    "                best_pc, best_shared, best_score = pc, shared_ct, sc\n",
    "\n",
    "        if best_pc is not None:\n",
    "            row = {\n",
    "                \"partd_row_id\": r[\"partd_row_id\"],\n",
    "                \"Gnrc_Name\": r[\"Gnrc_Name\"],\n",
    "                \"Gnrc_Name_clean\": r[\"Gnrc_Name_clean\"],\n",
    "                \"matched_primary_clean\": best_pc,\n",
    "                \"match_type\": \"generic_simple\",\n",
    "                \"match_score\": best_score,\n",
    "            }\n",
    "            for c in spend_cols:\n",
    "                row[c] = r[c]\n",
    "            rows.append(row)\n",
    "\n",
    "    simple_hits = pd.DataFrame(rows) if rows else pd.DataFrame(\n",
    "        columns=[\"partd_row_id\",\"Gnrc_Name\",\"Gnrc_Name_clean\",\n",
    "                 \"matched_primary_clean\",\"match_type\",\"match_score\"] + spend_cols\n",
    "    )\n",
    "\n",
    "    # combine\n",
    "    hits = pd.concat(\n",
    "        [\n",
    "            exact_hits[[\"partd_row_id\",\"Gnrc_Name\",\"Gnrc_Name_clean\",\n",
    "                        \"matched_primary_clean\",\"match_type\",\"match_score\"] + spend_cols],\n",
    "            simple_hits[[\"partd_row_id\",\"Gnrc_Name\",\"Gnrc_Name_clean\",\n",
    "                         \"matched_primary_clean\",\"match_type\",\"match_score\"] + spend_cols],\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    return hits\n",
    "\n",
    "# ---------------- TWO-PASS MATCHING ----------------\n",
    "# Pass 1: PRIORITY pool (new_small==1 OR new_bio==1)\n",
    "priority_pool = df.loc[(df[\"new_small\"]==1) | (df[\"new_bio\"]==1)].copy()\n",
    "general_pool  = df.loc[~df.index.isin(priority_pool.index)].copy()\n",
    "\n",
    "partd_remaining = partd.copy()\n",
    "\n",
    "pass1_hits = run_pass(partd_remaining, priority_pool, pass_tag=\"priority\")\n",
    "matched_ids_pass1 = set(pass1_hits[\"partd_row_id\"])\n",
    "\n",
    "# remove matched Part D rows before Pass 2\n",
    "partd_remaining = partd_remaining.loc[~partd_remaining[\"partd_row_id\"].isin(matched_ids_pass1)].copy()\n",
    "\n",
    "# Pass 2: GENERAL pool (leftover DF)\n",
    "pass2_hits = run_pass(partd_remaining, general_pool, pass_tag=\"general\")\n",
    "matched_ids_pass2 = set(pass2_hits[\"partd_row_id\"])\n",
    "\n",
    "# ---------------- STEP 3: TOKEN-CONTAINMENT (UNMATCHED indications vs leftover Part D) ----------------\n",
    "# Indications already matched in Pass 1+2:\n",
    "matched_primary_pass12 = set(\n",
    "    pd.concat([pass1_hits[\"matched_primary_clean\"], pass2_hits[\"matched_primary_clean\"]]).dropna().unique()\n",
    ")\n",
    "\n",
    "# Candidate indications for Step 3 = UNMATCHED only\n",
    "df_unmatched_pool = df.loc[~df[\"primary_clean\"].isin(matched_primary_pass12)].copy()\n",
    "\n",
    "# Leftover Part D rows after Pass 2\n",
    "leftover = partd_remaining.loc[~partd_remaining[\"partd_row_id\"].isin(matched_ids_pass2)].copy()\n",
    "\n",
    "def token_containment_pass(partd_leftover: pd.DataFrame, df_candidates: pd.DataFrame):\n",
    "    if partd_leftover.empty or df_candidates.empty:\n",
    "        return pd.DataFrame(columns=[\"partd_row_id\",\"Gnrc_Name\",\"Gnrc_Name_clean\",\n",
    "                                     \"matched_primary_clean\",\"match_type\",\"match_score\"] + spend_cols)\n",
    "\n",
    "    # Map token -> candidate primary_clean\n",
    "    token_to_primary = defaultdict(set)\n",
    "    primary_set_map  = dict(zip(df_candidates[\"primary_clean\"], df_candidates[\"primary_tokset\"]))\n",
    "    for pc, pset in primary_set_map.items():\n",
    "        for t in pset:\n",
    "            token_to_primary[t].add(pc)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in partd_leftover.iterrows():\n",
    "        gset = set(r[\"gnrc_tokset\"])\n",
    "        cand_primaries = set()\n",
    "        for t in gset:\n",
    "            cand_primaries |= token_to_primary.get(t, set())\n",
    "        if not cand_primaries:\n",
    "            continue\n",
    "\n",
    "        best_pc, best_shared, best_score = None, -1, 0\n",
    "        for pc in cand_primaries:\n",
    "            pset = primary_set_map[pc]\n",
    "            shared = len(gset & pset)\n",
    "            if shared == 0:\n",
    "                continue\n",
    "            sc = fuzz.token_set_ratio(r[\"Gnrc_Name_clean\"], pc)\n",
    "            if (shared > best_shared) or (shared == best_shared and sc > best_score):\n",
    "                best_pc, best_shared, best_score = pc, shared, sc\n",
    "\n",
    "        if best_pc is not None:\n",
    "            row = {\n",
    "                \"partd_row_id\": r[\"partd_row_id\"],\n",
    "                \"Gnrc_Name\": r[\"Gnrc_Name\"],\n",
    "                \"Gnrc_Name_clean\": r[\"Gnrc_Name_clean\"],\n",
    "                \"matched_primary_clean\": best_pc,\n",
    "                \"match_type\": \"token_contains\",\n",
    "                \"match_score\": best_score,\n",
    "            }\n",
    "            for c in spend_cols:\n",
    "                row[c] = r[c]\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "token_hits = token_containment_pass(leftover, df_unmatched_pool)\n",
    "\n",
    "# ---------------- COMBINE ALL MATCHES ----------------\n",
    "matches = pd.concat([pass1_hits, pass2_hits, token_hits], ignore_index=True)\n",
    "\n",
    "# ---------------- ATTACH drugid ----------------\n",
    "matches = matches.merge(\n",
    "    df[[\"primary_clean\",\"drugid\"]].drop_duplicates(),\n",
    "    left_on=\"matched_primary_clean\",\n",
    "    right_on=\"primary_clean\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"primary_clean\"])\n",
    "\n",
    "matches[\"matched_primary_name\"] = matches[\"matched_primary_clean\"].map(primary_to_name)\n",
    "matches.to_excel(out_matches, index=False)\n",
    "\n",
    "# ---------------- SUM spending BY drugid ----------------\n",
    "if spend_cols:\n",
    "    spend_by_drugid = (\n",
    "        matches.dropna(subset=[\"drugid\"])[[\"drugid\"] + spend_cols]\n",
    "               .groupby(\"drugid\", as_index=False)\n",
    "               .sum(min_count=1)\n",
    "    )\n",
    "else:\n",
    "    spend_by_drugid = pd.DataFrame(columns=[\"drugid\"])\n",
    "spend_by_drugid.to_excel(out_spend_by_drugid, index=False)\n",
    "\n",
    "# ---------------- MERGE totals back to your panel by drugid ----------------\n",
    "df_with_totals = df.merge(spend_by_drugid, on=\"drugid\", how=\"left\")\n",
    "df_with_totals.to_excel(out_panel, index=False)\n",
    "\n",
    "# ---------- Quick QA ----------\n",
    "exact_ct_1   = (pass1_hits[\"match_type\"] == \"generic_exact\").sum()\n",
    "simple_ct_1  = (pass1_hits[\"match_type\"] == \"generic_simple\").sum()\n",
    "exact_ct_2   = (pass2_hits[\"match_type\"] == \"generic_exact\").sum()\n",
    "simple_ct_2  = (pass2_hits[\"match_type\"] == \"generic_simple\").sum()\n",
    "token_ct     = (token_hits[\"match_type\"] == \"token_contains\").sum()\n",
    "\n",
    "matched_ids_all = pd.concat(\n",
    "    [pass1_hits[\"partd_row_id\"], pass2_hits[\"partd_row_id\"], token_hits[\"partd_row_id\"]]\n",
    ").nunique()\n",
    "unmatched_ct = len(partd) - matched_ids_all\n",
    "mapped_ids   = matches[\"drugid\"].nunique(dropna=True)\n",
    "\n",
    "print(\"---- PASS 1 (priority: new_small/new_bio) ----\")\n",
    "print(f\"Exact matches: {exact_ct_1:,}\")\n",
    "print(f\"Simple matches (score>=79 & max shared tokens): {simple_ct_1:,}\")\n",
    "print(\"---- PASS 2 (general leftovers) ----\")\n",
    "print(f\"Exact matches: {exact_ct_2:,}\")\n",
    "print(f\"Simple matches (score>=79 & max shared tokens): {simple_ct_2:,}\")\n",
    "print(\"---- PASS 3 (token containment, UNMATCHED indications) ----\")\n",
    "print(f\"Token-contained matches: {token_ct:,}\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Unmatched Part D rows: {unmatched_ct:,}\")\n",
    "print(f\"Mapped unique drugids: {mapped_ids:,}\")\n",
    "print(f\"Final panel written to: {out_panel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac03c65-cd46-4629-adec-bb74515af1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- PATHS ----------------\n",
    "path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_with_partd_totals_by_drugid.xlsx\"\n",
    "output_path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_partd_approval.xlsx\"\n",
    "\n",
    "# ---------------- LOAD ----------------\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "# ---------------- COLUMN NAMES ----------------\n",
    "drug_col = \"drugid\"\n",
    "etype_col = \"eventtype\"\n",
    "edet_col  = \"eventdetails\"\n",
    "date_col  = \"eventdate\"   # you said the date column is eventdate\n",
    "\n",
    "# ---------------- NORMALIZE DATE ----------------\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "\n",
    "# ---------------- \"US\" DETECTOR ----------------\n",
    "# Exact “The US” (case-insensitive). If you want to be more flexible, see the alt regex below.\n",
    "US_RE = re.compile(r\"\\bThe\\s*US\\b\", flags=re.IGNORECASE)\n",
    "# Flexible alternative (comment the line above and uncomment the line below if desired):\n",
    "# US_RE = re.compile(r\"\\b(?:The\\s*US|US|U\\.S\\.A?|USA|United\\s+States)\\b\", re.IGNORECASE)\n",
    "\n",
    "# ---------------- INDICATION COUNT (AFTER \"The US\") ----------------\n",
    "def count_after_us_items(text) -> int:\n",
    "    \"\"\"\n",
    "    After 'The US', strip leading/trailing separators.\n",
    "    If tail is non-empty and has NO semicolons -> return 1.\n",
    "    Otherwise -> return the count of semicolons.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "\n",
    "    m = US_RE.search(text)\n",
    "    if not m:\n",
    "        return 0\n",
    "\n",
    "    # tail after 'The US'\n",
    "    tail = text[m.end():]\n",
    "\n",
    "    # strip leading and trailing separators (incl. leading/trailing ';')\n",
    "    tail = re.sub(r'^[\\s:–—\\-;]+', '', tail)   # leading\n",
    "    tail = re.sub(r'[\\s;.,]+$', '', tail)      # trailing\n",
    "\n",
    "    if not tail:\n",
    "        return 0\n",
    "\n",
    "    semi_count = tail.count(';')\n",
    "    return 1 if semi_count == 0 else semi_count\n",
    "\n",
    "\n",
    "mask_us   = df[edet_col].astype(str).str.contains(US_RE, na=False)\n",
    "mask_appr = df[etype_col].astype(str).str.contains(\"Approval\", case=False, na=False)\n",
    "\n",
    "# create/overwrite indication_count\n",
    "df[\"indication_count\"] = 0\n",
    "df.loc[mask_us & mask_appr, \"indication_count\"] = (\n",
    "    df.loc[mask_us & mask_appr, edet_col].apply(count_after_us_items)\n",
    ")\n",
    "\n",
    "# ---------------- FIRST DATES BY DRUGID (US-only) ----------------\n",
    "# Approval and Launch masks\n",
    "mask_launch = df[etype_col].astype(str).str.contains(\"Launch\", case=False, na=False)\n",
    "\n",
    "# Earliest US+Approval per drug\n",
    "fa_by_id = (\n",
    "    df.loc[mask_us & mask_appr, [drug_col, date_col]]\n",
    "      .groupby(drug_col, dropna=False)[date_col]\n",
    "      .min()\n",
    "      .rename(\"first_approval_date\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Earliest US+Launch per drug\n",
    "fl_by_id = (\n",
    "    df.loc[mask_us & mask_launch, [drug_col, date_col]]\n",
    "      .groupby(drug_col, dropna=False)[date_col]\n",
    "      .min()\n",
    "      .rename(\"first_launch_date\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Merge back to main df\n",
    "for col in [\"first_approval_date\", \"first_launch_date\", \"first_approval_or_launch\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = pd.NaT\n",
    "\n",
    "df = df.drop(columns=[\"first_approval_date\", \"first_launch_date\", \"first_approval_or_launch\"], errors=\"ignore\")\n",
    "df = df.merge(fa_by_id, on=drug_col, how=\"left\")\n",
    "df = df.merge(fl_by_id, on=drug_col, how=\"left\")\n",
    "\n",
    "# approval-or-launch preference: approval if present, else launch\n",
    "df[\"first_approval_or_launch\"] = df[\"first_approval_date\"].where(\n",
    "    df[\"first_approval_date\"].notna(),\n",
    "    df[\"first_launch_date\"]\n",
    ")\n",
    "\n",
    "# ---------------- SAVE ----------------\n",
    "df.to_excel(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2c1235-7ecd-448a-b7f7-8f53db47555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- PATHS ----------------\n",
    "path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_partd_approval.xlsx\"\n",
    "output_path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_partd_approval_trial.xlsx\"\n",
    "\n",
    "# ---------------- LOAD ----------------\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "# ---------------- COL GUARDS ----------------\n",
    "# handle occasional misspelling 'eventdetials'\n",
    "details_col = \"eventdetails\"\n",
    "type_col = \"eventtype\"\n",
    "\n",
    "#base mask (no capturing groups)\n",
    "base_mask = df[type_col].astype(str).str.contains(\n",
    "    r'(?:Global Status Advance|Disease Phase Change)', case=False, na=False, regex=True\n",
    ")\n",
    "\n",
    "# compile patterns with NON-capturing groups\n",
    "NDR_RE       = re.compile(r'\\b(?:NDR|No\\s*Devel)\\b', re.IGNORECASE)\n",
    "TRIAL_RE     = re.compile(r'\\btrials?\\b', re.IGNORECASE)  # no group needed\n",
    "\n",
    "PHASE_I_RE   = re.compile(r'\\b(?:ph(?:ase)?\\s*i|ph(?:ase)?\\s*1)\\b',   re.IGNORECASE)\n",
    "PHASE_II_RE  = re.compile(r'\\b(?:ph(?:ase)?\\s*ii|ph(?:ase)?\\s*2)\\b',  re.IGNORECASE)\n",
    "PHASE_III_RE = re.compile(r'\\b(?:ph(?:ase)?\\s*iii|ph(?:ase)?\\s*3)\\b', re.IGNORECASE)\n",
    "\n",
    "det = df[details_col].astype(str)\n",
    "\n",
    "df[\"NDR\"]      = (base_mask & det.str.contains(NDR_RE,       na=False)).astype(int)\n",
    "df[\"trial\"]    = (base_mask & det.str.contains(TRIAL_RE,     na=False)).astype(int)\n",
    "df[\"phasei\"]   = (base_mask & det.str.contains(PHASE_I_RE,   na=False)).astype(int)\n",
    "df[\"phaseii\"]  = (base_mask & det.str.contains(PHASE_II_RE,  na=False)).astype(int)\n",
    "df[\"phaseiii\"] = (base_mask & det.str.contains(PHASE_III_RE, na=False)).astype(int)\n",
    "\n",
    "# ---------------- COLLAPSE LOGIC ----------------\n",
    "# If Phase III is present, zero out Phase I and Phase II (since Phase III implies earlier phases)\n",
    "df.loc[df[\"phaseiii\"] == 1, [\"phasei\", \"phaseii\"]] = 0\n",
    "df.loc[df[\"phaseii\"] == 1, [\"phasei\"]] = 0\n",
    "\n",
    "df[\"other\"] = 0\n",
    "bucket_cols = [\"NDR\", \"trial\", \"phasei\", \"phaseii\", \"phaseiii\"]\n",
    "df.loc[base_mask & (df[bucket_cols].sum(axis=1) == 0), \"other\"] = 1\n",
    "\n",
    "phase_cols = [\"phasei\", \"phaseii\", \"phaseiii\"]\n",
    "df[\"trial\"] = ((df[\"trial\"] == 1) | df[phase_cols].any(axis=1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d858298b-0eef-4b1c-9ecf-138b996e9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yizho\\AppData\\Local\\Temp\\ipykernel_1412\\1847737857.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_cum_from_anchor, col=\"indication_count\")\n",
      "C:\\Users\\yizho\\AppData\\Local\\Temp\\ipykernel_1412\\1847737857.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_cum_from_anchor, col=\"trial\")\n",
      "C:\\Users\\yizho\\AppData\\Local\\Temp\\ipykernel_1412\\1847737857.py:38: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_cum_from_anchor, col=\"phase_any\")\n",
      "C:\\Users\\yizho\\AppData\\Local\\Temp\\ipykernel_1412\\1847737857.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[\"cum_phasei\"]   = df.groupby(\"drugid\", group_keys=False).apply(_cum_from_anchor, col=\"phasei\")\n",
      "C:\\Users\\yizho\\AppData\\Local\\Temp\\ipykernel_1412\\1847737857.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[\"cum_phaseii\"]  = df.groupby(\"drugid\", group_keys=False).apply(_cum_from_anchor, col=\"phaseii\")\n",
      "C:\\Users\\yizho\\AppData\\Local\\Temp\\ipykernel_1412\\1847737857.py:44: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[\"cum_phaseiii\"] = df.groupby(\"drugid\", group_keys=False).apply(_cum_from_anchor, col=\"phaseiii\")\n"
     ]
    }
   ],
   "source": [
    "date_col = \"eventdate\"\n",
    "\n",
    "# --- datetimes & sort ---\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"first_approval_date\"] = pd.to_datetime(df[\"first_approval_date\"], errors=\"coerce\")\n",
    "df = df.sort_values([\"drugid\", date_col], kind=\"mergesort\")\n",
    "\n",
    "# --- use the existing first_approval_date per drugid and broadcast it ---\n",
    "# (assumes exactly one non-null per drugid; transform('max') copies that value to all rows)\n",
    "anchor = df.groupby(\"drugid\")[\"first_approval_date\"].transform(\"max\")\n",
    "\n",
    "# --- rows to count in cumulatives: on/after anchor & anchor exists ---\n",
    "df[\"counting_mask\"] = anchor.notna() & (df[date_col] >= anchor)\n",
    "\n",
    "# --- any phase at row ---\n",
    "df[\"phase_any\"] = ((df[\"phasei\"] == 1) | (df[\"phaseii\"] == 1) | (df[\"phaseiii\"] == 1)).astype(int)\n",
    "\n",
    "# --- cumulative helper (within each drugid) ---\n",
    "def _cum_from_anchor(g: pd.DataFrame, col: str) -> pd.Series:\n",
    "    vals = g[col].fillna(0).where(g[\"counting_mask\"], 0)\n",
    "    return vals.cumsum()\n",
    "\n",
    "# cumulative indication_count (from first approval onward)\n",
    "df[\"cum_indications\"] = (\n",
    "    df.groupby(\"drugid\", group_keys=False)\n",
    "      .apply(_cum_from_anchor, col=\"indication_count\")\n",
    ")\n",
    "\n",
    "# cumulative trials\n",
    "df[\"cum_trials\"] = (\n",
    "    df.groupby(\"drugid\", group_keys=False)\n",
    "      .apply(_cum_from_anchor, col=\"trial\")\n",
    ")\n",
    "\n",
    "# cumulative phases (any phase)\n",
    "df[\"cum_phases\"] = (\n",
    "    df.groupby(\"drugid\", group_keys=False)\n",
    "      .apply(_cum_from_anchor, col=\"phase_any\")\n",
    ")\n",
    "\n",
    "# (Optional) per-phase cumulatives\n",
    "df[\"cum_phasei\"]   = df.groupby(\"drugid\", group_keys=False).apply(_cum_from_anchor, col=\"phasei\")\n",
    "df[\"cum_phaseii\"]  = df.groupby(\"drugid\", group_keys=False).apply(_cum_from_anchor, col=\"phaseii\")\n",
    "df[\"cum_phaseiii\"] = df.groupby(\"drugid\", group_keys=False).apply(_cum_from_anchor, col=\"phaseiii\")\n",
    "\n",
    "# flag: drug has a non-empty first_approval_date\n",
    "df[\"has_first_approval_date\"] = anchor.notna().astype(int)\n",
    "\n",
    "# SAVE\n",
    "df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc881fcf-c71f-4480-ac19-3ea8e7a59a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- PATHS ----------------\n",
    "path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_partd_approval_trial.xlsx\"\n",
    "output_path = r\"D:\\PhD\\RA\\Schafer\\IRA\\data\\transformed\\indications_partd_approval_trial_yearly.xlsx\"\n",
    "\n",
    "# ---------------- LOAD ----------------\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "# ---------------- DATES ----------------\n",
    "date_col = \"eventdate\"\n",
    "if date_col not in df.columns:\n",
    "    raise KeyError(f\"Missing required date column: {date_col}\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"first_approval_date\"] = pd.to_datetime(df.get(\"first_approval_date\"), errors=\"coerce\")\n",
    "\n",
    "# ---------------- INDICATIONS @ APPROVAL (per drug) ----------------\n",
    "# anchor per row = existing first_approval_date for that drug (broadcast)\n",
    "anchor = df.groupby(\"drugid\")[\"first_approval_date\"].transform(\"max\")\n",
    "\n",
    "# mark rows that fall on the first approval date (compare by date, ignore time)\n",
    "is_app_day = anchor.notna() & (df[date_col].dt.normalize() == anchor.dt.normalize())\n",
    "\n",
    "# sum indication_count on the approval date per drugid, then merge back\n",
    "if \"indication_count\" not in df.columns:\n",
    "    raise KeyError(\"Missing required column: indication_count\")\n",
    "\n",
    "ind_at_app = (\n",
    "    df.loc[is_app_day, [\"drugid\", \"indication_count\"]]\n",
    "      .groupby(\"drugid\", as_index=False)[\"indication_count\"].sum()\n",
    "      .rename(columns={\"indication_count\": \"indications_at_app\"})\n",
    ")\n",
    "\n",
    "df = df.merge(ind_at_app, on=\"drugid\", how=\"left\")\n",
    "df[\"indications_at_app\"] = df[\"indications_at_app\"].fillna(0).astype(int)\n",
    "\n",
    "# ---------------- YEAR + APPYEAR ----------------\n",
    "df[\"year\"] = df[date_col].dt.year\n",
    "# ensure datetime then derive appyear\n",
    "df[\"first_approval_date\"] = pd.to_datetime(df.get(\"first_approval_date\"), errors=\"coerce\")\n",
    "df[\"appyear\"] = df[\"first_approval_date\"].dt.year\n",
    "\n",
    "# ---------------- LAST OBSERVATION PER (drugid, year) ----------------\n",
    "# Sort then keep the last row within each (drugid, year)\n",
    "df_sorted = df.sort_values([\"drugid\", \"year\", date_col], kind=\"mergesort\")\n",
    "year_last = df_sorted.drop_duplicates(subset=[\"drugid\", \"year\"], keep=\"last\").copy()\n",
    "\n",
    "# ---------------- EXPAND YEARS FROM APPYEAR TO 2025 ----------------\n",
    "END_YEAR = 2025\n",
    "\n",
    "# We only expand for drugids with a non-null appyear that is <= END_YEAR\n",
    "base = year_last.copy()\n",
    "valid = base.dropna(subset=[\"appyear\"]).query(\"appyear <= @END_YEAR\")[[\"drugid\", \"appyear\"]].drop_duplicates()\n",
    "\n",
    "# Build a full grid of (drugid, year) from appyear..END_YEAR for each valid drug\n",
    "grids = []\n",
    "for rid, appy in valid.itertuples(index=False):\n",
    "    start = int(appy)\n",
    "    if start > END_YEAR:\n",
    "        continue\n",
    "    yrs = np.arange(start, END_YEAR + 1, dtype=int)\n",
    "    grids.append(pd.DataFrame({\"drugid\": rid, \"year\": yrs, \"appyear\": start}))\n",
    "full_grid = pd.concat(grids, ignore_index=True) if grids else pd.DataFrame(columns=[\"drugid\",\"year\",\"appyear\"])\n",
    "\n",
    "# Merge grid with the last-obs rows; then forward-fill within each drugid by year\n",
    "year_panel = full_grid.merge(base, on=[\"drugid\", \"year\"], how=\"left\", suffixes=(\"\", \"_orig\"))\n",
    "\n",
    "# Ensure rows are ordered to ffill correctly\n",
    "year_panel = year_panel.sort_values([\"drugid\", \"year\"], kind=\"mergesort\")\n",
    "\n",
    "# Columns we should forward fill. By default, carry forward *everything* except the keys and raw eventdate.\n",
    "# If you want to limit to specific columns, list them explicitly instead.\n",
    "dont_ffill = {\"drugid\", \"year\", \"appyear\"}  # keep appyear as constant; it's already present from the grid\n",
    "if date_col in year_panel.columns:\n",
    "    dont_ffill.add(date_col)\n",
    "\n",
    "ffill_cols = [c for c in year_panel.columns if c not in dont_ffill]\n",
    "\n",
    "year_panel[ffill_cols] = (\n",
    "    year_panel.groupby(\"drugid\", as_index=False)[ffill_cols]\n",
    "              .ffill()\n",
    ")\n",
    "\n",
    "# For drugids without any appyear (NaN), keep their observed (drugid, year) rows as-is\n",
    "# Append non-expanded drugs (if any)\n",
    "no_app = base.loc[base[\"appyear\"].isna(), :].copy()\n",
    "if not no_app.empty:\n",
    "    # We already kept last obs per (drugid, year) in `base`; just append those rows\n",
    "    # (no expansion since appyear is missing)\n",
    "    keep_cols = year_panel.columns\n",
    "    no_app = no_app.reindex(columns=keep_cols)\n",
    "    year_panel = pd.concat([year_panel, no_app], ignore_index=True)\n",
    "\n",
    "# Sort final panel\n",
    "year_panel = year_panel.sort_values([\"drugid\", \"year\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# Optional: make sure types look good\n",
    "year_panel[\"indications_at_app\"] = year_panel[\"indications_at_app\"].fillna(0).astype(int)\n",
    "# If you have cumulative cols (e.g., cum_indications, cum_trials…), they’ll be carried forward as well.\n",
    "\n",
    "\n",
    "\n",
    "# 1) Identify and coerce Part D spend columns\n",
    "spend_cols = [c for c in year_panel.columns if str(c).startswith(\"Tot_Spndng_\")]\n",
    "if spend_cols:\n",
    "    year_panel[spend_cols] = year_panel[spend_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# 2) Per-drug 2021 spend and rank (higher spend -> rank 1)\n",
    "spend_2021_col = \"Tot_Spndng_2021\"\n",
    "\n",
    "drug_spend_2021 = (\n",
    "    year_panel.groupby(\"drugid\", as_index=False)[spend_2021_col]\n",
    "              .max()  # same within drug; max ignores NaN\n",
    "              .rename(columns={spend_2021_col: \"spend_2021\"})\n",
    ")\n",
    "\n",
    "drug_spend_2021[\"rank_spend_2021\"] = (\n",
    "    drug_spend_2021[\"spend_2021\"]\n",
    "        .rank(method=\"dense\", ascending=False, na_option=\"bottom\")\n",
    "        .astype(\"Int64\")\n",
    ")\n",
    "\n",
    "# 3) Merge ranks back to every row of that drug\n",
    "year_panel = year_panel.merge(\n",
    "    drug_spend_2021[[\"drugid\", \"spend_2021\", \"rank_spend_2021\"]],\n",
    "    on=\"drugid\", how=\"left\"\n",
    ")\n",
    "\n",
    "# 4) Top-N dummies\n",
    "year_panel[\"top50\"]  = year_panel[\"rank_spend_2021\"].le(50).astype(\"Int64\")\n",
    "year_panel[\"top100\"] = year_panel[\"rank_spend_2021\"].le(100).astype(\"Int64\")\n",
    "year_panel[\"top250\"] = year_panel[\"rank_spend_2021\"].le(250).astype(\"Int64\")\n",
    "\n",
    "# 5) partd flag = 1 if ANY Part D spend column is non-empty for this drug (broadcast to all its rows)\n",
    "if spend_cols:\n",
    "    row_has_spend = year_panel[spend_cols].notna().any(axis=1)\n",
    "    year_panel[\"partd\"] = row_has_spend.groupby(year_panel[\"drugid\"]).transform(\"max\").astype(int)\n",
    "else:\n",
    "    year_panel[\"partd\"] = 0\n",
    "    \n",
    "# ---------------- SAVE ----------------\n",
    "year_panel.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3487a-3c9a-4808-84d2-e6a627c78bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
